{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "631a6c7a-6963-4099-ba45-ec0ece1df492",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import gensim.models\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from collections import Counter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39964fbc-1fb0-450b-92e7-b22784087a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "newmodel = gensim.models.KeyedVectors.load_word2vec_format('reducedvector.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e04819f0-771e-4c8e-9ca1-3f25de562acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.5876938104629517), ('girl', 0.5229198932647705), ('young', 0.49715912342071533), ('immortal', 0.4890766441822052), ('spider', 0.47930291295051575)]\n",
      "0.5876938\n",
      "[('queen', 0.5532454252243042)]\n"
     ]
    }
   ],
   "source": [
    "# taken from homework instructions for testing and examples\n",
    "# Find the five nearest neighbors to the word man\n",
    "print(newmodel.most_similar('man', topn=5))\n",
    "\n",
    "# Compute a measure of similarity between woman and man\n",
    "print(newmodel.similarity('woman', 'man'))\n",
    "\n",
    "# To complete analogies like man is to woman as king is to ??, we can use:\n",
    "print(newmodel.most_similar(positive=['king', 'woman'], negative=['man'], topn=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19c2bbe5-7e92-4abb-9ff8-23889c3f1484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity_scores_dataframe(word2vec_model, target_words_list, comparison_words_list):\n",
    "    \"\"\"\n",
    "    Computes similarity scores between target words and comparison words using a Word2Vec model.\n",
    "\n",
    "    Parameters:\n",
    "        model (gensim.models.KeyedVectors): The pre-trained Word2Vec model, or any future created word model\n",
    "        target_words_list (list of str): List of targeting words to compare against.\n",
    "        comparison_words_list (list of str): List of words to compute similarity for or file\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame of similarity scores.\n",
    "    \"\"\"\n",
    "     # Initialize dictionary to hold similarity results\n",
    "    word_similarity_results_dict = {'Comparison_Word': comparison_words_list}\n",
    "    \n",
    "    # Loop through each target word (e.g., 'man', 'woman')\n",
    "    for target_word_comparing in target_words_list:\n",
    "        similarity_scores_list = []\n",
    "        \n",
    "        # Loop through each word to compare with the target\n",
    "        for comparison_word_checking in comparison_words_list:\n",
    "            try:\n",
    "                similarity_score_value = word2vec_model.similarity(target_word_comparing, comparison_word_checking)\n",
    "            except KeyError:\n",
    "                similarity_score_value = None  # In case a word is missing from vocabulary\n",
    "            \n",
    "            similarity_scores_list.append(similarity_score_value)\n",
    "        \n",
    "        # Label column based on the target word\n",
    "        column_label_name = f'Similarity_to_{target_word_comparing}'\n",
    "        word_similarity_results_dict[column_label_name] = similarity_scores_list\n",
    "\n",
    "    # Convert dictionary to DataFrame\n",
    "    word_similarity_dataframe = pd.DataFrame(word_similarity_results_dict)\n",
    "    \n",
    "    return word_similarity_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e14880d6-7a8f-4ef8-9b47-18fba5821e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity rankings based off of 'man':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comparison_Word</th>\n",
       "      <th>Similarity_to_man</th>\n",
       "      <th>Similarity_to_woman</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>man</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.587694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>woman</td>\n",
       "      <td>0.587694</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>child</td>\n",
       "      <td>0.333422</td>\n",
       "      <td>0.589809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doctor</td>\n",
       "      <td>0.289247</td>\n",
       "      <td>0.196134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wife</td>\n",
       "      <td>0.283479</td>\n",
       "      <td>0.300689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>king</td>\n",
       "      <td>0.264497</td>\n",
       "      <td>0.122529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>husband</td>\n",
       "      <td>0.234116</td>\n",
       "      <td>0.449643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>nurse</td>\n",
       "      <td>0.153481</td>\n",
       "      <td>0.254358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>birth</td>\n",
       "      <td>0.123439</td>\n",
       "      <td>0.420309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>scientist</td>\n",
       "      <td>0.112269</td>\n",
       "      <td>0.137311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>queen</td>\n",
       "      <td>0.110419</td>\n",
       "      <td>0.228572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>professor</td>\n",
       "      <td>0.107622</td>\n",
       "      <td>0.105199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>teacher</td>\n",
       "      <td>0.098740</td>\n",
       "      <td>0.204078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>president</td>\n",
       "      <td>0.094579</td>\n",
       "      <td>0.084627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>engineer</td>\n",
       "      <td>0.087364</td>\n",
       "      <td>0.044264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Comparison_Word  Similarity_to_man  Similarity_to_woman\n",
       "0              man           1.000000             0.587694\n",
       "1            woman           0.587694             1.000000\n",
       "2            child           0.333422             0.589809\n",
       "3           doctor           0.289247             0.196134\n",
       "4             wife           0.283479             0.300689\n",
       "5             king           0.264497             0.122529\n",
       "6          husband           0.234116             0.449643\n",
       "7            nurse           0.153481             0.254358\n",
       "8            birth           0.123439             0.420309\n",
       "9        scientist           0.112269             0.137311\n",
       "10           queen           0.110419             0.228572\n",
       "11       professor           0.107622             0.105199\n",
       "12         teacher           0.098740             0.204078\n",
       "13       president           0.094579             0.084627\n",
       "14        engineer           0.087364             0.044264"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity rankings based off of 'woman':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comparison_Word</th>\n",
       "      <th>Similarity_to_man</th>\n",
       "      <th>Similarity_to_woman</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>woman</td>\n",
       "      <td>0.587694</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>child</td>\n",
       "      <td>0.333422</td>\n",
       "      <td>0.589809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>man</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.587694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>husband</td>\n",
       "      <td>0.234116</td>\n",
       "      <td>0.449643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>birth</td>\n",
       "      <td>0.123439</td>\n",
       "      <td>0.420309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>wife</td>\n",
       "      <td>0.283479</td>\n",
       "      <td>0.300689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nurse</td>\n",
       "      <td>0.153481</td>\n",
       "      <td>0.254358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>queen</td>\n",
       "      <td>0.110419</td>\n",
       "      <td>0.228572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>teacher</td>\n",
       "      <td>0.098740</td>\n",
       "      <td>0.204078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>doctor</td>\n",
       "      <td>0.289247</td>\n",
       "      <td>0.196134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>scientist</td>\n",
       "      <td>0.112269</td>\n",
       "      <td>0.137311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>king</td>\n",
       "      <td>0.264497</td>\n",
       "      <td>0.122529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>professor</td>\n",
       "      <td>0.107622</td>\n",
       "      <td>0.105199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>president</td>\n",
       "      <td>0.094579</td>\n",
       "      <td>0.084627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>engineer</td>\n",
       "      <td>0.087364</td>\n",
       "      <td>0.044264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Comparison_Word  Similarity_to_man  Similarity_to_woman\n",
       "0            woman           0.587694             1.000000\n",
       "1            child           0.333422             0.589809\n",
       "2              man           1.000000             0.587694\n",
       "3          husband           0.234116             0.449643\n",
       "4            birth           0.123439             0.420309\n",
       "5             wife           0.283479             0.300689\n",
       "6            nurse           0.153481             0.254358\n",
       "7            queen           0.110419             0.228572\n",
       "8          teacher           0.098740             0.204078\n",
       "9           doctor           0.289247             0.196134\n",
       "10       scientist           0.112269             0.137311\n",
       "11            king           0.264497             0.122529\n",
       "12       professor           0.107622             0.105199\n",
       "13       president           0.094579             0.084627\n",
       "14        engineer           0.087364             0.044264"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Question 1: Define the target words to be compared againsts\n",
    "target_words_list = ['man', 'woman']\n",
    "\n",
    "# Define the words we will compute similarity scores for each word in the list\n",
    "comparison_words_list = [\n",
    "    'wife', 'husband', 'child', 'queen', 'king',\n",
    "    'man', 'woman', 'birth', 'doctor', 'nurse',\n",
    "    'teacher', 'professor', 'engineer', 'scientist', 'president'\n",
    "]\n",
    "\n",
    "# Run function using your preloaded newmodel\n",
    "word_similarity_dataframe = compute_similarity_scores_dataframe(newmodel, target_words_list, comparison_words_list)\n",
    "\n",
    "# Create sorted DataFrames by similarity to 'man' and 'woman'\n",
    "sorted_by_man_similarity_df = word_similarity_dataframe.sort_values(\n",
    "    by='Similarity_to_man', ascending=False\n",
    ").reset_index(drop=True)\n",
    "\n",
    "sorted_by_woman_similarity_df = word_similarity_dataframe.sort_values(\n",
    "    by='Similarity_to_woman', ascending=False\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Display results\n",
    "print(\"Similarity rankings based off of 'man':\")\n",
    "display(sorted_by_man_similarity_df)\n",
    "\n",
    "print(\"Similarity rankings based off of 'woman':\")\n",
    "display(sorted_by_woman_similarity_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f19832da-2643-4e8e-8aed-8a11f5f7ea1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load E04 txt and extract comparison list into comparison words list object\n",
    "comparison_words_list = []\n",
    "with open('BATS_3.0/3_Encyclopedic_semantics/E04 [name - nationality].txt', 'r') as bats_national_file:\n",
    "    for line in bats_national_file:\n",
    "        words_combos = line.strip().lower().split() #ensures that the data is consistent to prevent comparison issues later\n",
    "        comparison_words_list.extend(words_combos)  # Adds the words in each row to the list\n",
    "\n",
    "# de-dupping the lists if extra sets exist\n",
    "comparison_words_list = list(set(comparison_words_list))\n",
    "\n",
    "# Pick some random target words to see what comes up\n",
    "target_words_list = ['american', 'korean', 'african']  # just an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ee55e25f-621e-4dae-b7d3-eeee119d99d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Similarity rankings based off of 'american':\n",
      "        Comparison_Word  Similarity_to_american  Similarity_to_korean  Similarity_to_african\n",
      "               american                1.000000              0.326140               0.391764\n",
      "                italian                0.412269              0.223306               0.190187\n",
      "                russian                0.399078              0.264673               0.218097\n",
      "                 french                0.364208              0.202297               0.236438\n",
      "                 german                0.362015              0.231492               0.143659\n",
      "               austrian                0.269358              0.108587               0.111678\n",
      "                 polish                0.233618              0.221085               0.177134\n",
      "                  dutch                0.232405              0.172685               0.246436\n",
      "            tchaikovsky                0.227087              0.028781              -0.027718\n",
      "                chinese                0.219128              0.623694               0.293512\n",
      "                   depp                0.174743             -0.019281               0.015936\n",
      "                 edison                0.173936             -0.094101              -0.063225\n",
      "            dostoyevsky                0.154242              0.025445              -0.028953\n",
      "                  greek                0.151800              0.215471               0.139611\n",
      "                raphael                0.136002             -0.063096              -0.039507\n",
      "                 balzac                0.128906             -0.039738               0.005431\n",
      "                lincoln                0.119771              0.086945               0.032171\n",
      "                 wagner                0.101193             -0.024553              -0.153213\n",
      "                  locke                0.094563             -0.142719              -0.079990\n",
      "                 darwin                0.090683             -0.015485               0.017095\n",
      "               rousseau                0.080512             -0.160249              -0.007692\n",
      "                  roman                0.080302              0.051124               0.121666\n",
      "                   hume                0.080041             -0.043076              -0.070849\n",
      "                 stalin                0.069655              0.167034               0.054163\n",
      "                 truman                0.066777              0.203394               0.041890\n",
      "                 lennon                0.066676             -0.118682              -0.038462\n",
      "                  jolie                0.061703             -0.014490              -0.065637\n",
      "                   marx                0.044696              0.027924              -0.029926\n",
      "                  homer                0.044337              0.086435              -0.059836\n",
      "                strauss                0.034181             -0.003562              -0.040890\n",
      "                 newton                0.031286             -0.151824              -0.068534\n",
      "                spinoza                0.027162             -0.187131              -0.095338\n",
      "                galilei                0.017077             -0.144722              -0.107011\n",
      "                  hegel                0.016072             -0.047926              -0.032635\n",
      "                dickens                0.015889             -0.136038              -0.053582\n",
      "                  lenin                0.009743             -0.028604               0.020118\n",
      "           michelangelo                0.008448             -0.078963              -0.073559\n",
      "              lavoisier               -0.002615             -0.135643              -0.087835\n",
      "               napoleon               -0.005638              0.019741              -0.042437\n",
      "                   kant               -0.007359             -0.088854              -0.134417\n",
      "             copernicus               -0.025102             -0.141709              -0.121381\n",
      "              confucius               -0.028457              0.081741              -0.035111\n",
      "            machiavelli               -0.030701             -0.155750              -0.053751\n",
      "                mencius               -0.030793              0.041331               0.009290\n",
      "              beethoven               -0.030840              0.031380              -0.110650\n",
      "                 hitler               -0.031608              0.135107              -0.011098\n",
      "                maxwell               -0.035087             -0.024829              -0.055608\n",
      "                 pascal               -0.036383              0.033893              -0.015741\n",
      "                leibniz               -0.040645             -0.108990              -0.110250\n",
      "                 kepler               -0.047154             -0.101764              -0.086268\n",
      "                hawking               -0.047439             -0.215733              -0.129463\n",
      "                 mozart               -0.056824             -0.038262              -0.170813\n",
      "                  plato               -0.061062             -0.064223               0.005630\n",
      "              gorbachev               -0.069398              0.028294               0.079375\n",
      "                 caesar               -0.081030              0.012573              -0.059009\n",
      "              aristotle               -0.084016             -0.110361              -0.077140\n",
      "                  fermi               -0.087210             -0.203030              -0.111099\n",
      "                 euclid               -0.114925             -0.094279              -0.079811\n",
      "              descartes               -0.116272             -0.170399              -0.030661\n",
      "               einstein               -0.123796             -0.127994              -0.073708\n",
      " jewish/german/american                     NaN                   NaN                    NaN\n",
      "                tolstoi                     NaN                   NaN                    NaN\n",
      "        german/austrian                     NaN                   NaN                    NaN\n",
      "       scottish/british                     NaN                   NaN                    NaN\n",
      "french/corsican/italian                     NaN                   NaN                    NaN\n",
      "        english/british                     NaN                   NaN                    NaN\n",
      "        soviet/georgian                     NaN                   NaN                    NaN\n",
      "         soviet/russian                     NaN                   NaN                    NaN\n",
      "\n",
      " Similarity rankings based off of 'korean':\n",
      "        Comparison_Word  Similarity_to_american  Similarity_to_korean  Similarity_to_african\n",
      "                chinese                0.219128              0.623694               0.293512\n",
      "               american                1.000000              0.326140               0.391764\n",
      "                russian                0.399078              0.264673               0.218097\n",
      "                 german                0.362015              0.231492               0.143659\n",
      "                italian                0.412269              0.223306               0.190187\n",
      "                 polish                0.233618              0.221085               0.177134\n",
      "                  greek                0.151800              0.215471               0.139611\n",
      "                 truman                0.066777              0.203394               0.041890\n",
      "                 french                0.364208              0.202297               0.236438\n",
      "                  dutch                0.232405              0.172685               0.246436\n",
      "                 stalin                0.069655              0.167034               0.054163\n",
      "                 hitler               -0.031608              0.135107              -0.011098\n",
      "               austrian                0.269358              0.108587               0.111678\n",
      "                lincoln                0.119771              0.086945               0.032171\n",
      "                  homer                0.044337              0.086435              -0.059836\n",
      "              confucius               -0.028457              0.081741              -0.035111\n",
      "                  roman                0.080302              0.051124               0.121666\n",
      "                mencius               -0.030793              0.041331               0.009290\n",
      "                 pascal               -0.036383              0.033893              -0.015741\n",
      "              beethoven               -0.030840              0.031380              -0.110650\n",
      "            tchaikovsky                0.227087              0.028781              -0.027718\n",
      "              gorbachev               -0.069398              0.028294               0.079375\n",
      "                   marx                0.044696              0.027924              -0.029926\n",
      "            dostoyevsky                0.154242              0.025445              -0.028953\n",
      "               napoleon               -0.005638              0.019741              -0.042437\n",
      "                 caesar               -0.081030              0.012573              -0.059009\n",
      "                strauss                0.034181             -0.003562              -0.040890\n",
      "                  jolie                0.061703             -0.014490              -0.065637\n",
      "                 darwin                0.090683             -0.015485               0.017095\n",
      "                   depp                0.174743             -0.019281               0.015936\n",
      "                 wagner                0.101193             -0.024553              -0.153213\n",
      "                maxwell               -0.035087             -0.024829              -0.055608\n",
      "                  lenin                0.009743             -0.028604               0.020118\n",
      "                 mozart               -0.056824             -0.038262              -0.170813\n",
      "                 balzac                0.128906             -0.039738               0.005431\n",
      "                   hume                0.080041             -0.043076              -0.070849\n",
      "                  hegel                0.016072             -0.047926              -0.032635\n",
      "                raphael                0.136002             -0.063096              -0.039507\n",
      "                  plato               -0.061062             -0.064223               0.005630\n",
      "           michelangelo                0.008448             -0.078963              -0.073559\n",
      "                   kant               -0.007359             -0.088854              -0.134417\n",
      "                 edison                0.173936             -0.094101              -0.063225\n",
      "                 euclid               -0.114925             -0.094279              -0.079811\n",
      "                 kepler               -0.047154             -0.101764              -0.086268\n",
      "                leibniz               -0.040645             -0.108990              -0.110250\n",
      "              aristotle               -0.084016             -0.110361              -0.077140\n",
      "                 lennon                0.066676             -0.118682              -0.038462\n",
      "               einstein               -0.123796             -0.127994              -0.073708\n",
      "              lavoisier               -0.002615             -0.135643              -0.087835\n",
      "                dickens                0.015889             -0.136038              -0.053582\n",
      "             copernicus               -0.025102             -0.141709              -0.121381\n",
      "                  locke                0.094563             -0.142719              -0.079990\n",
      "                galilei                0.017077             -0.144722              -0.107011\n",
      "                 newton                0.031286             -0.151824              -0.068534\n",
      "            machiavelli               -0.030701             -0.155750              -0.053751\n",
      "               rousseau                0.080512             -0.160249              -0.007692\n",
      "              descartes               -0.116272             -0.170399              -0.030661\n",
      "                spinoza                0.027162             -0.187131              -0.095338\n",
      "                  fermi               -0.087210             -0.203030              -0.111099\n",
      "                hawking               -0.047439             -0.215733              -0.129463\n",
      " jewish/german/american                     NaN                   NaN                    NaN\n",
      "                tolstoi                     NaN                   NaN                    NaN\n",
      "        german/austrian                     NaN                   NaN                    NaN\n",
      "       scottish/british                     NaN                   NaN                    NaN\n",
      "french/corsican/italian                     NaN                   NaN                    NaN\n",
      "        english/british                     NaN                   NaN                    NaN\n",
      "        soviet/georgian                     NaN                   NaN                    NaN\n",
      "         soviet/russian                     NaN                   NaN                    NaN\n",
      "\n",
      " Similarity rankings based off of 'african':\n",
      "        Comparison_Word  Similarity_to_american  Similarity_to_korean  Similarity_to_african\n",
      "               american                1.000000              0.326140               0.391764\n",
      "                chinese                0.219128              0.623694               0.293512\n",
      "                  dutch                0.232405              0.172685               0.246436\n",
      "                 french                0.364208              0.202297               0.236438\n",
      "                russian                0.399078              0.264673               0.218097\n",
      "                italian                0.412269              0.223306               0.190187\n",
      "                 polish                0.233618              0.221085               0.177134\n",
      "                 german                0.362015              0.231492               0.143659\n",
      "                  greek                0.151800              0.215471               0.139611\n",
      "                  roman                0.080302              0.051124               0.121666\n",
      "               austrian                0.269358              0.108587               0.111678\n",
      "              gorbachev               -0.069398              0.028294               0.079375\n",
      "                 stalin                0.069655              0.167034               0.054163\n",
      "                 truman                0.066777              0.203394               0.041890\n",
      "                lincoln                0.119771              0.086945               0.032171\n",
      "                  lenin                0.009743             -0.028604               0.020118\n",
      "                 darwin                0.090683             -0.015485               0.017095\n",
      "                   depp                0.174743             -0.019281               0.015936\n",
      "                mencius               -0.030793              0.041331               0.009290\n",
      "                  plato               -0.061062             -0.064223               0.005630\n",
      "                 balzac                0.128906             -0.039738               0.005431\n",
      "               rousseau                0.080512             -0.160249              -0.007692\n",
      "                 hitler               -0.031608              0.135107              -0.011098\n",
      "                 pascal               -0.036383              0.033893              -0.015741\n",
      "            tchaikovsky                0.227087              0.028781              -0.027718\n",
      "            dostoyevsky                0.154242              0.025445              -0.028953\n",
      "                   marx                0.044696              0.027924              -0.029926\n",
      "              descartes               -0.116272             -0.170399              -0.030661\n",
      "                  hegel                0.016072             -0.047926              -0.032635\n",
      "              confucius               -0.028457              0.081741              -0.035111\n",
      "                 lennon                0.066676             -0.118682              -0.038462\n",
      "                raphael                0.136002             -0.063096              -0.039507\n",
      "                strauss                0.034181             -0.003562              -0.040890\n",
      "               napoleon               -0.005638              0.019741              -0.042437\n",
      "                dickens                0.015889             -0.136038              -0.053582\n",
      "            machiavelli               -0.030701             -0.155750              -0.053751\n",
      "                maxwell               -0.035087             -0.024829              -0.055608\n",
      "                 caesar               -0.081030              0.012573              -0.059009\n",
      "                  homer                0.044337              0.086435              -0.059836\n",
      "                 edison                0.173936             -0.094101              -0.063225\n",
      "                  jolie                0.061703             -0.014490              -0.065637\n",
      "                 newton                0.031286             -0.151824              -0.068534\n",
      "                   hume                0.080041             -0.043076              -0.070849\n",
      "           michelangelo                0.008448             -0.078963              -0.073559\n",
      "               einstein               -0.123796             -0.127994              -0.073708\n",
      "              aristotle               -0.084016             -0.110361              -0.077140\n",
      "                 euclid               -0.114925             -0.094279              -0.079811\n",
      "                  locke                0.094563             -0.142719              -0.079990\n",
      "                 kepler               -0.047154             -0.101764              -0.086268\n",
      "              lavoisier               -0.002615             -0.135643              -0.087835\n",
      "                spinoza                0.027162             -0.187131              -0.095338\n",
      "                galilei                0.017077             -0.144722              -0.107011\n",
      "                leibniz               -0.040645             -0.108990              -0.110250\n",
      "              beethoven               -0.030840              0.031380              -0.110650\n",
      "                  fermi               -0.087210             -0.203030              -0.111099\n",
      "             copernicus               -0.025102             -0.141709              -0.121381\n",
      "                hawking               -0.047439             -0.215733              -0.129463\n",
      "                   kant               -0.007359             -0.088854              -0.134417\n",
      "                 wagner                0.101193             -0.024553              -0.153213\n",
      "                 mozart               -0.056824             -0.038262              -0.170813\n",
      " jewish/german/american                     NaN                   NaN                    NaN\n",
      "                tolstoi                     NaN                   NaN                    NaN\n",
      "        german/austrian                     NaN                   NaN                    NaN\n",
      "       scottish/british                     NaN                   NaN                    NaN\n",
      "french/corsican/italian                     NaN                   NaN                    NaN\n",
      "        english/british                     NaN                   NaN                    NaN\n",
      "        soviet/georgian                     NaN                   NaN                    NaN\n",
      "         soviet/russian                     NaN                   NaN                    NaN\n"
     ]
    }
   ],
   "source": [
    "# make the word similarity df\n",
    "word_similarity_dataframe = compute_similarity_scores_dataframe(\n",
    "    word2vec_model=newmodel,\n",
    "    target_words_list=target_words_list,\n",
    "    comparison_words_list=comparison_words_list\n",
    ")\n",
    "\n",
    "# Sort and display by one of the target classes for demonstration purposes\n",
    "for target in target_words_list:\n",
    "    print(f\"\\n Similarity rankings made based off of '{target}':\")\n",
    "    sorted_df = word_similarity_dataframe.sort_values(\n",
    "        by=f'Similarity_to_{target}', ascending=False\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    print(sorted_df.to_string(index=False))  # print all rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1b44c26f-71be-4cca-85ff-1b62cb46c1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bats_similarity_dataframe_custom(model_wordvec, bats_analogy_rows_list, protected_class_words_list=None):\n",
    "    \"\"\"\n",
    "    Computes similarity scores between the first word in each BATS analogy row\n",
    "    and the other words in that row, optionally comparing to protected class words.\n",
    "    \n",
    "    Parameters:\n",
    "        model_wordvec (gensim KeyedVectors): Pre-trained Word2Vec model.\n",
    "        bats_analogy_rows_list (list of lists): Each list contains 4 related words from a BATS analogy file.\n",
    "        protected_class_words_list (list of str): Words related to a protected class (optional).\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame showing similarity scores.\n",
    "    \"\"\"\n",
    "    similarity_results_dict_list = []  # Initialize list to hold similarity result dictionaries\n",
    "    \n",
    "    for analogy_row_words_group in bats_analogy_rows_list:  # Loop through each analogy word group (usually 4 words)\n",
    "        target_primary_word = analogy_row_words_group[0]  # Always select the first word in analogy group as the target\n",
    "\n",
    "        # Compare the target word to each of the remaining analogy words\n",
    "        for comparison_analogy_word in analogy_row_words_group[1:]:\n",
    "            try:\n",
    "                similarity_score_value = model_wordvec.similarity(target_primary_word, comparison_analogy_word)  # Compute similarity\n",
    "            except KeyError:\n",
    "                similarity_score_value = None  # Handle missing words (not in model vocab)\n",
    "            \n",
    "            # Store result as dictionary, with 'Type' field indicating it's part of the analogy set\n",
    "            similarity_results_dict_list.append({\n",
    "                'Target_Word': target_primary_word,  # The focus word in this comparison\n",
    "                'Comparison_Word': comparison_analogy_word,  # The word being compared to the target\n",
    "                'Similarity_Score': similarity_score_value,  # Cosine similarity value or None\n",
    "                'Type': 'Analogy_Row'  # Label this as a normal analogy comparison\n",
    "            })\n",
    "\n",
    "        # If protected class words are provided, compare each to the target word\n",
    "        if protected_class_words_list:\n",
    "            for protected_reference_word in protected_class_words_list:\n",
    "                try:\n",
    "                    similarity_score_value = model_wordvec.similarity(target_primary_word, protected_reference_word)  # Same similarity computation\n",
    "                except KeyError:\n",
    "                    similarity_score_value = None  # If missing, skip it but store as None\n",
    "                    \n",
    "                # Add the protected class comparison to the result list\n",
    "                similarity_results_dict_list.append({\n",
    "                    'Target_Word': target_primary_word,  # Again, same target word\n",
    "                    'Comparison_Word': protected_reference_word,  # A protected group word\n",
    "                    'Similarity_Score': similarity_score_value,  # Sim score (can be None if word not foundâ€”whoops)\n",
    "                    'Type': 'Protected_Comparison'  # Differentiate this as a protected class evaluation\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(similarity_results_dict_list)  # Convert final list to DataFrame for easy tabular inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "76ff0c36-16f8-4724-860e-0f6419898028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATS National Origin Protected Class Similarities:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Line_Number</th>\n",
       "      <th>Target_Word</th>\n",
       "      <th>Row_Words</th>\n",
       "      <th>Sim_to_greek</th>\n",
       "      <th>Sim_to_Protected_korean</th>\n",
       "      <th>Sim_to_Protected_african</th>\n",
       "      <th>Sim_to_Protected_american</th>\n",
       "      <th>Noticeable_Protected_Class_Sim_Diff</th>\n",
       "      <th>Sim_to_french</th>\n",
       "      <th>Sim_to_german</th>\n",
       "      <th>...</th>\n",
       "      <th>Sim_to_russian</th>\n",
       "      <th>Sim_to_jewish/german/american</th>\n",
       "      <th>Sim_to_italian</th>\n",
       "      <th>Sim_to_soviet/russian</th>\n",
       "      <th>Sim_to_german/austrian</th>\n",
       "      <th>Sim_to_scottish/british</th>\n",
       "      <th>Sim_to_french/corsican/italian</th>\n",
       "      <th>Sim_to_dutch</th>\n",
       "      <th>Sim_to_soviet/georgian</th>\n",
       "      <th>Sim_to_austrian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>aristotle</td>\n",
       "      <td>[aristotle, greek]</td>\n",
       "      <td>0.276697</td>\n",
       "      <td>-0.110361</td>\n",
       "      <td>-0.077140</td>\n",
       "      <td>-0.084016</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>balzac</td>\n",
       "      <td>[balzac, french]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.039738</td>\n",
       "      <td>0.005431</td>\n",
       "      <td>0.128906</td>\n",
       "      <td>True</td>\n",
       "      <td>0.238700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>beethoven</td>\n",
       "      <td>[beethoven, german]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.031380</td>\n",
       "      <td>-0.110650</td>\n",
       "      <td>-0.030840</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.216786</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>caesar</td>\n",
       "      <td>[caesar, roman]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.012573</td>\n",
       "      <td>-0.059009</td>\n",
       "      <td>-0.081030</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>confucius</td>\n",
       "      <td>[confucius, chinese]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.081741</td>\n",
       "      <td>-0.035111</td>\n",
       "      <td>-0.028457</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>copernicus</td>\n",
       "      <td>[copernicus, polish]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.141709</td>\n",
       "      <td>-0.121381</td>\n",
       "      <td>-0.025102</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>darwin</td>\n",
       "      <td>[darwin, english/british]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.015485</td>\n",
       "      <td>0.017095</td>\n",
       "      <td>0.090683</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>depp</td>\n",
       "      <td>[depp, american]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.019281</td>\n",
       "      <td>0.015936</td>\n",
       "      <td>0.174743</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>descartes</td>\n",
       "      <td>[descartes, french]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.170399</td>\n",
       "      <td>-0.030661</td>\n",
       "      <td>-0.116272</td>\n",
       "      <td>True</td>\n",
       "      <td>0.117955</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>dickens</td>\n",
       "      <td>[dickens, english/british]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.136038</td>\n",
       "      <td>-0.053582</td>\n",
       "      <td>0.015889</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>dostoyevsky</td>\n",
       "      <td>[dostoyevsky, russian]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.025445</td>\n",
       "      <td>-0.028953</td>\n",
       "      <td>0.154242</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148206</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>edison</td>\n",
       "      <td>[edison, american]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.094101</td>\n",
       "      <td>-0.063225</td>\n",
       "      <td>0.173936</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>einstein</td>\n",
       "      <td>[einstein, jewish/german/american]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.127994</td>\n",
       "      <td>-0.073708</td>\n",
       "      <td>-0.123796</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>euclid</td>\n",
       "      <td>[euclid, greek]</td>\n",
       "      <td>0.083233</td>\n",
       "      <td>-0.094279</td>\n",
       "      <td>-0.079811</td>\n",
       "      <td>-0.114925</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>fermi</td>\n",
       "      <td>[fermi, italian]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.203030</td>\n",
       "      <td>-0.111099</td>\n",
       "      <td>-0.087210</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.020734</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>galilei</td>\n",
       "      <td>[galilei, italian]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.144722</td>\n",
       "      <td>-0.107011</td>\n",
       "      <td>0.017077</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.158951</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>gorbachev</td>\n",
       "      <td>[gorbachev, soviet/russian]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.028294</td>\n",
       "      <td>0.079375</td>\n",
       "      <td>-0.069398</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>hawking</td>\n",
       "      <td>[hawking, english/british]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.215733</td>\n",
       "      <td>-0.129463</td>\n",
       "      <td>-0.047439</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>hegel</td>\n",
       "      <td>[hegel, german]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.047926</td>\n",
       "      <td>-0.032635</td>\n",
       "      <td>0.016072</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.201735</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>hitler</td>\n",
       "      <td>[hitler, german/austrian]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.135107</td>\n",
       "      <td>-0.011098</td>\n",
       "      <td>-0.031608</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>homer</td>\n",
       "      <td>[homer, greek]</td>\n",
       "      <td>0.318008</td>\n",
       "      <td>0.086435</td>\n",
       "      <td>-0.059836</td>\n",
       "      <td>0.044337</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>hume</td>\n",
       "      <td>[hume, scottish/british]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.043076</td>\n",
       "      <td>-0.070849</td>\n",
       "      <td>0.080041</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>jolie</td>\n",
       "      <td>[jolie, american]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.014490</td>\n",
       "      <td>-0.065637</td>\n",
       "      <td>0.061703</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>kant</td>\n",
       "      <td>[kant, german]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.088854</td>\n",
       "      <td>-0.134417</td>\n",
       "      <td>-0.007359</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.096969</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>kepler</td>\n",
       "      <td>[kepler, german]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.101764</td>\n",
       "      <td>-0.086268</td>\n",
       "      <td>-0.047154</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.024587</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>lavoisier</td>\n",
       "      <td>[lavoisier, french]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.135643</td>\n",
       "      <td>-0.087835</td>\n",
       "      <td>-0.002615</td>\n",
       "      <td>True</td>\n",
       "      <td>0.112806</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>leibniz</td>\n",
       "      <td>[leibniz, german]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.108990</td>\n",
       "      <td>-0.110250</td>\n",
       "      <td>-0.040645</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.166256</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>lenin</td>\n",
       "      <td>[lenin, soviet/russian]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.028604</td>\n",
       "      <td>0.020118</td>\n",
       "      <td>0.009743</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>lennon</td>\n",
       "      <td>[lennon, english/british]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.118682</td>\n",
       "      <td>-0.038462</td>\n",
       "      <td>0.066676</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>lincoln</td>\n",
       "      <td>[lincoln, american]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.086945</td>\n",
       "      <td>0.032171</td>\n",
       "      <td>0.119771</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>locke</td>\n",
       "      <td>[locke, english/british]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.142719</td>\n",
       "      <td>-0.079990</td>\n",
       "      <td>0.094563</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>machiavelli</td>\n",
       "      <td>[machiavelli, italian]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.155750</td>\n",
       "      <td>-0.053751</td>\n",
       "      <td>-0.030701</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.025799</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>marx</td>\n",
       "      <td>[marx, german]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.027924</td>\n",
       "      <td>-0.029926</td>\n",
       "      <td>0.044696</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.255866</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>maxwell</td>\n",
       "      <td>[maxwell, scottish/british]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.024829</td>\n",
       "      <td>-0.055608</td>\n",
       "      <td>-0.035087</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>mencius</td>\n",
       "      <td>[mencius, chinese]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.041331</td>\n",
       "      <td>0.009290</td>\n",
       "      <td>-0.030793</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>36</td>\n",
       "      <td>michelangelo</td>\n",
       "      <td>[michelangelo, italian]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.078963</td>\n",
       "      <td>-0.073559</td>\n",
       "      <td>0.008448</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.243113</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>37</td>\n",
       "      <td>mozart</td>\n",
       "      <td>[mozart, german/austrian]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.038262</td>\n",
       "      <td>-0.170813</td>\n",
       "      <td>-0.056824</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>38</td>\n",
       "      <td>napoleon</td>\n",
       "      <td>[napoleon, french/corsican/italian]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.019741</td>\n",
       "      <td>-0.042437</td>\n",
       "      <td>-0.005638</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>newton</td>\n",
       "      <td>[newton, english/british]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.151824</td>\n",
       "      <td>-0.068534</td>\n",
       "      <td>0.031286</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40</td>\n",
       "      <td>pascal</td>\n",
       "      <td>[pascal, french]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.033893</td>\n",
       "      <td>-0.015741</td>\n",
       "      <td>-0.036383</td>\n",
       "      <td>False</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>41</td>\n",
       "      <td>plato</td>\n",
       "      <td>[plato, greek]</td>\n",
       "      <td>0.343930</td>\n",
       "      <td>-0.064223</td>\n",
       "      <td>0.005630</td>\n",
       "      <td>-0.061062</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>42</td>\n",
       "      <td>raphael</td>\n",
       "      <td>[raphael, italian]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.063096</td>\n",
       "      <td>-0.039507</td>\n",
       "      <td>0.136002</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.247711</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>43</td>\n",
       "      <td>rousseau</td>\n",
       "      <td>[rousseau, french]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.160249</td>\n",
       "      <td>-0.007692</td>\n",
       "      <td>0.080512</td>\n",
       "      <td>True</td>\n",
       "      <td>0.190979</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>44</td>\n",
       "      <td>spinoza</td>\n",
       "      <td>[spinoza, dutch]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.187131</td>\n",
       "      <td>-0.095338</td>\n",
       "      <td>0.027162</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.019125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>45</td>\n",
       "      <td>stalin</td>\n",
       "      <td>[stalin, soviet/georgian]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.167034</td>\n",
       "      <td>0.054163</td>\n",
       "      <td>0.069655</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>46</td>\n",
       "      <td>strauss</td>\n",
       "      <td>[strauss, austrian]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.003562</td>\n",
       "      <td>-0.040890</td>\n",
       "      <td>0.034181</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.24934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>47</td>\n",
       "      <td>tchaikovsky</td>\n",
       "      <td>[tchaikovsky, russian]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.028781</td>\n",
       "      <td>-0.027718</td>\n",
       "      <td>0.227087</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160950</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>48</td>\n",
       "      <td>tolstoi</td>\n",
       "      <td>[tolstoi, russian]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>49</td>\n",
       "      <td>truman</td>\n",
       "      <td>[truman, american]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.203394</td>\n",
       "      <td>0.041890</td>\n",
       "      <td>0.066777</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>50</td>\n",
       "      <td>wagner</td>\n",
       "      <td>[wagner, german]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.024553</td>\n",
       "      <td>-0.153213</td>\n",
       "      <td>0.101193</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.247726</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Line_Number   Target_Word                            Row_Words  \\\n",
       "0             1     aristotle                   [aristotle, greek]   \n",
       "1             2        balzac                     [balzac, french]   \n",
       "2             3     beethoven                  [beethoven, german]   \n",
       "3             4        caesar                      [caesar, roman]   \n",
       "4             5     confucius                 [confucius, chinese]   \n",
       "5             6    copernicus                 [copernicus, polish]   \n",
       "6             7        darwin            [darwin, english/british]   \n",
       "7             8          depp                     [depp, american]   \n",
       "8             9     descartes                  [descartes, french]   \n",
       "9            10       dickens           [dickens, english/british]   \n",
       "10           11   dostoyevsky               [dostoyevsky, russian]   \n",
       "11           12        edison                   [edison, american]   \n",
       "12           13      einstein   [einstein, jewish/german/american]   \n",
       "13           14        euclid                      [euclid, greek]   \n",
       "14           15         fermi                     [fermi, italian]   \n",
       "15           16       galilei                   [galilei, italian]   \n",
       "16           17     gorbachev          [gorbachev, soviet/russian]   \n",
       "17           18       hawking           [hawking, english/british]   \n",
       "18           19         hegel                      [hegel, german]   \n",
       "19           20        hitler            [hitler, german/austrian]   \n",
       "20           21         homer                       [homer, greek]   \n",
       "21           22          hume             [hume, scottish/british]   \n",
       "22           23         jolie                    [jolie, american]   \n",
       "23           24          kant                       [kant, german]   \n",
       "24           25        kepler                     [kepler, german]   \n",
       "25           26     lavoisier                  [lavoisier, french]   \n",
       "26           27       leibniz                    [leibniz, german]   \n",
       "27           28         lenin              [lenin, soviet/russian]   \n",
       "28           29        lennon            [lennon, english/british]   \n",
       "29           30       lincoln                  [lincoln, american]   \n",
       "30           31         locke             [locke, english/british]   \n",
       "31           32   machiavelli               [machiavelli, italian]   \n",
       "32           33          marx                       [marx, german]   \n",
       "33           34       maxwell          [maxwell, scottish/british]   \n",
       "34           35       mencius                   [mencius, chinese]   \n",
       "35           36  michelangelo              [michelangelo, italian]   \n",
       "36           37        mozart            [mozart, german/austrian]   \n",
       "37           38      napoleon  [napoleon, french/corsican/italian]   \n",
       "38           39        newton            [newton, english/british]   \n",
       "39           40        pascal                     [pascal, french]   \n",
       "40           41         plato                       [plato, greek]   \n",
       "41           42       raphael                   [raphael, italian]   \n",
       "42           43      rousseau                   [rousseau, french]   \n",
       "43           44       spinoza                     [spinoza, dutch]   \n",
       "44           45        stalin            [stalin, soviet/georgian]   \n",
       "45           46       strauss                  [strauss, austrian]   \n",
       "46           47   tchaikovsky               [tchaikovsky, russian]   \n",
       "47           48       tolstoi                   [tolstoi, russian]   \n",
       "48           49        truman                   [truman, american]   \n",
       "49           50        wagner                     [wagner, german]   \n",
       "\n",
       "    Sim_to_greek  Sim_to_Protected_korean  Sim_to_Protected_african  \\\n",
       "0       0.276697                -0.110361                 -0.077140   \n",
       "1            NaN                -0.039738                  0.005431   \n",
       "2            NaN                 0.031380                 -0.110650   \n",
       "3            NaN                 0.012573                 -0.059009   \n",
       "4            NaN                 0.081741                 -0.035111   \n",
       "5            NaN                -0.141709                 -0.121381   \n",
       "6            NaN                -0.015485                  0.017095   \n",
       "7            NaN                -0.019281                  0.015936   \n",
       "8            NaN                -0.170399                 -0.030661   \n",
       "9            NaN                -0.136038                 -0.053582   \n",
       "10           NaN                 0.025445                 -0.028953   \n",
       "11           NaN                -0.094101                 -0.063225   \n",
       "12           NaN                -0.127994                 -0.073708   \n",
       "13      0.083233                -0.094279                 -0.079811   \n",
       "14           NaN                -0.203030                 -0.111099   \n",
       "15           NaN                -0.144722                 -0.107011   \n",
       "16           NaN                 0.028294                  0.079375   \n",
       "17           NaN                -0.215733                 -0.129463   \n",
       "18           NaN                -0.047926                 -0.032635   \n",
       "19           NaN                 0.135107                 -0.011098   \n",
       "20      0.318008                 0.086435                 -0.059836   \n",
       "21           NaN                -0.043076                 -0.070849   \n",
       "22           NaN                -0.014490                 -0.065637   \n",
       "23           NaN                -0.088854                 -0.134417   \n",
       "24           NaN                -0.101764                 -0.086268   \n",
       "25           NaN                -0.135643                 -0.087835   \n",
       "26           NaN                -0.108990                 -0.110250   \n",
       "27           NaN                -0.028604                  0.020118   \n",
       "28           NaN                -0.118682                 -0.038462   \n",
       "29           NaN                 0.086945                  0.032171   \n",
       "30           NaN                -0.142719                 -0.079990   \n",
       "31           NaN                -0.155750                 -0.053751   \n",
       "32           NaN                 0.027924                 -0.029926   \n",
       "33           NaN                -0.024829                 -0.055608   \n",
       "34           NaN                 0.041331                  0.009290   \n",
       "35           NaN                -0.078963                 -0.073559   \n",
       "36           NaN                -0.038262                 -0.170813   \n",
       "37           NaN                 0.019741                 -0.042437   \n",
       "38           NaN                -0.151824                 -0.068534   \n",
       "39           NaN                 0.033893                 -0.015741   \n",
       "40      0.343930                -0.064223                  0.005630   \n",
       "41           NaN                -0.063096                 -0.039507   \n",
       "42           NaN                -0.160249                 -0.007692   \n",
       "43           NaN                -0.187131                 -0.095338   \n",
       "44           NaN                 0.167034                  0.054163   \n",
       "45           NaN                -0.003562                 -0.040890   \n",
       "46           NaN                 0.028781                 -0.027718   \n",
       "47           NaN                      NaN                       NaN   \n",
       "48           NaN                 0.203394                  0.041890   \n",
       "49           NaN                -0.024553                 -0.153213   \n",
       "\n",
       "    Sim_to_Protected_american  Noticeable_Protected_Class_Sim_Diff  \\\n",
       "0                   -0.084016                                False   \n",
       "1                    0.128906                                 True   \n",
       "2                   -0.030840                                 True   \n",
       "3                   -0.081030                                False   \n",
       "4                   -0.028457                                 True   \n",
       "5                   -0.025102                                 True   \n",
       "6                    0.090683                                 True   \n",
       "7                    0.174743                                 True   \n",
       "8                   -0.116272                                 True   \n",
       "9                    0.015889                                 True   \n",
       "10                   0.154242                                 True   \n",
       "11                   0.173936                                 True   \n",
       "12                  -0.123796                                False   \n",
       "13                  -0.114925                                False   \n",
       "14                  -0.087210                                 True   \n",
       "15                   0.017077                                 True   \n",
       "16                  -0.069398                                 True   \n",
       "17                  -0.047439                                 True   \n",
       "18                   0.016072                                False   \n",
       "19                  -0.031608                                 True   \n",
       "20                   0.044337                                 True   \n",
       "21                   0.080041                                 True   \n",
       "22                   0.061703                                 True   \n",
       "23                  -0.007359                                 True   \n",
       "24                  -0.047154                                False   \n",
       "25                  -0.002615                                 True   \n",
       "26                  -0.040645                                False   \n",
       "27                   0.009743                                False   \n",
       "28                   0.066676                                 True   \n",
       "29                   0.119771                                False   \n",
       "30                   0.094563                                 True   \n",
       "31                  -0.030701                                 True   \n",
       "32                   0.044696                                False   \n",
       "33                  -0.035087                                False   \n",
       "34                  -0.030793                                False   \n",
       "35                   0.008448                                False   \n",
       "36                  -0.056824                                 True   \n",
       "37                  -0.005638                                False   \n",
       "38                   0.031286                                 True   \n",
       "39                  -0.036383                                False   \n",
       "40                  -0.061062                                False   \n",
       "41                   0.136002                                 True   \n",
       "42                   0.080512                                 True   \n",
       "43                   0.027162                                 True   \n",
       "44                   0.069655                                 True   \n",
       "45                   0.034181                                False   \n",
       "46                   0.227087                                 True   \n",
       "47                        NaN                                False   \n",
       "48                   0.066777                                 True   \n",
       "49                   0.101193                                 True   \n",
       "\n",
       "    Sim_to_french  Sim_to_german  ...  Sim_to_russian  \\\n",
       "0             NaN            NaN  ...             NaN   \n",
       "1        0.238700            NaN  ...             NaN   \n",
       "2             NaN       0.216786  ...             NaN   \n",
       "3             NaN            NaN  ...             NaN   \n",
       "4             NaN            NaN  ...             NaN   \n",
       "5             NaN            NaN  ...             NaN   \n",
       "6             NaN            NaN  ...             NaN   \n",
       "7             NaN            NaN  ...             NaN   \n",
       "8        0.117955            NaN  ...             NaN   \n",
       "9             NaN            NaN  ...             NaN   \n",
       "10            NaN            NaN  ...        0.148206   \n",
       "11            NaN            NaN  ...             NaN   \n",
       "12            NaN            NaN  ...             NaN   \n",
       "13            NaN            NaN  ...             NaN   \n",
       "14            NaN            NaN  ...             NaN   \n",
       "15            NaN            NaN  ...             NaN   \n",
       "16            NaN            NaN  ...             NaN   \n",
       "17            NaN            NaN  ...             NaN   \n",
       "18            NaN       0.201735  ...             NaN   \n",
       "19            NaN            NaN  ...             NaN   \n",
       "20            NaN            NaN  ...             NaN   \n",
       "21            NaN            NaN  ...             NaN   \n",
       "22            NaN            NaN  ...             NaN   \n",
       "23            NaN       0.096969  ...             NaN   \n",
       "24            NaN       0.024587  ...             NaN   \n",
       "25       0.112806            NaN  ...             NaN   \n",
       "26            NaN       0.166256  ...             NaN   \n",
       "27            NaN            NaN  ...             NaN   \n",
       "28            NaN            NaN  ...             NaN   \n",
       "29            NaN            NaN  ...             NaN   \n",
       "30            NaN            NaN  ...             NaN   \n",
       "31            NaN            NaN  ...             NaN   \n",
       "32            NaN       0.255866  ...             NaN   \n",
       "33            NaN            NaN  ...             NaN   \n",
       "34            NaN            NaN  ...             NaN   \n",
       "35            NaN            NaN  ...             NaN   \n",
       "36            NaN            NaN  ...             NaN   \n",
       "37            NaN            NaN  ...             NaN   \n",
       "38            NaN            NaN  ...             NaN   \n",
       "39       0.007500            NaN  ...             NaN   \n",
       "40            NaN            NaN  ...             NaN   \n",
       "41            NaN            NaN  ...             NaN   \n",
       "42       0.190979            NaN  ...             NaN   \n",
       "43            NaN            NaN  ...             NaN   \n",
       "44            NaN            NaN  ...             NaN   \n",
       "45            NaN            NaN  ...             NaN   \n",
       "46            NaN            NaN  ...        0.160950   \n",
       "47            NaN            NaN  ...             NaN   \n",
       "48            NaN            NaN  ...             NaN   \n",
       "49            NaN       0.247726  ...             NaN   \n",
       "\n",
       "    Sim_to_jewish/german/american  Sim_to_italian  Sim_to_soviet/russian  \\\n",
       "0                             NaN             NaN                    NaN   \n",
       "1                             NaN             NaN                    NaN   \n",
       "2                             NaN             NaN                    NaN   \n",
       "3                             NaN             NaN                    NaN   \n",
       "4                             NaN             NaN                    NaN   \n",
       "5                             NaN             NaN                    NaN   \n",
       "6                             NaN             NaN                    NaN   \n",
       "7                             NaN             NaN                    NaN   \n",
       "8                             NaN             NaN                    NaN   \n",
       "9                             NaN             NaN                    NaN   \n",
       "10                            NaN             NaN                    NaN   \n",
       "11                            NaN             NaN                    NaN   \n",
       "12                            NaN             NaN                    NaN   \n",
       "13                            NaN             NaN                    NaN   \n",
       "14                            NaN       -0.020734                    NaN   \n",
       "15                            NaN        0.158951                    NaN   \n",
       "16                            NaN             NaN                    NaN   \n",
       "17                            NaN             NaN                    NaN   \n",
       "18                            NaN             NaN                    NaN   \n",
       "19                            NaN             NaN                    NaN   \n",
       "20                            NaN             NaN                    NaN   \n",
       "21                            NaN             NaN                    NaN   \n",
       "22                            NaN             NaN                    NaN   \n",
       "23                            NaN             NaN                    NaN   \n",
       "24                            NaN             NaN                    NaN   \n",
       "25                            NaN             NaN                    NaN   \n",
       "26                            NaN             NaN                    NaN   \n",
       "27                            NaN             NaN                    NaN   \n",
       "28                            NaN             NaN                    NaN   \n",
       "29                            NaN             NaN                    NaN   \n",
       "30                            NaN             NaN                    NaN   \n",
       "31                            NaN        0.025799                    NaN   \n",
       "32                            NaN             NaN                    NaN   \n",
       "33                            NaN             NaN                    NaN   \n",
       "34                            NaN             NaN                    NaN   \n",
       "35                            NaN        0.243113                    NaN   \n",
       "36                            NaN             NaN                    NaN   \n",
       "37                            NaN             NaN                    NaN   \n",
       "38                            NaN             NaN                    NaN   \n",
       "39                            NaN             NaN                    NaN   \n",
       "40                            NaN             NaN                    NaN   \n",
       "41                            NaN        0.247711                    NaN   \n",
       "42                            NaN             NaN                    NaN   \n",
       "43                            NaN             NaN                    NaN   \n",
       "44                            NaN             NaN                    NaN   \n",
       "45                            NaN             NaN                    NaN   \n",
       "46                            NaN             NaN                    NaN   \n",
       "47                            NaN             NaN                    NaN   \n",
       "48                            NaN             NaN                    NaN   \n",
       "49                            NaN             NaN                    NaN   \n",
       "\n",
       "    Sim_to_german/austrian  Sim_to_scottish/british  \\\n",
       "0                      NaN                      NaN   \n",
       "1                      NaN                      NaN   \n",
       "2                      NaN                      NaN   \n",
       "3                      NaN                      NaN   \n",
       "4                      NaN                      NaN   \n",
       "5                      NaN                      NaN   \n",
       "6                      NaN                      NaN   \n",
       "7                      NaN                      NaN   \n",
       "8                      NaN                      NaN   \n",
       "9                      NaN                      NaN   \n",
       "10                     NaN                      NaN   \n",
       "11                     NaN                      NaN   \n",
       "12                     NaN                      NaN   \n",
       "13                     NaN                      NaN   \n",
       "14                     NaN                      NaN   \n",
       "15                     NaN                      NaN   \n",
       "16                     NaN                      NaN   \n",
       "17                     NaN                      NaN   \n",
       "18                     NaN                      NaN   \n",
       "19                     NaN                      NaN   \n",
       "20                     NaN                      NaN   \n",
       "21                     NaN                      NaN   \n",
       "22                     NaN                      NaN   \n",
       "23                     NaN                      NaN   \n",
       "24                     NaN                      NaN   \n",
       "25                     NaN                      NaN   \n",
       "26                     NaN                      NaN   \n",
       "27                     NaN                      NaN   \n",
       "28                     NaN                      NaN   \n",
       "29                     NaN                      NaN   \n",
       "30                     NaN                      NaN   \n",
       "31                     NaN                      NaN   \n",
       "32                     NaN                      NaN   \n",
       "33                     NaN                      NaN   \n",
       "34                     NaN                      NaN   \n",
       "35                     NaN                      NaN   \n",
       "36                     NaN                      NaN   \n",
       "37                     NaN                      NaN   \n",
       "38                     NaN                      NaN   \n",
       "39                     NaN                      NaN   \n",
       "40                     NaN                      NaN   \n",
       "41                     NaN                      NaN   \n",
       "42                     NaN                      NaN   \n",
       "43                     NaN                      NaN   \n",
       "44                     NaN                      NaN   \n",
       "45                     NaN                      NaN   \n",
       "46                     NaN                      NaN   \n",
       "47                     NaN                      NaN   \n",
       "48                     NaN                      NaN   \n",
       "49                     NaN                      NaN   \n",
       "\n",
       "    Sim_to_french/corsican/italian  Sim_to_dutch  Sim_to_soviet/georgian  \\\n",
       "0                              NaN           NaN                     NaN   \n",
       "1                              NaN           NaN                     NaN   \n",
       "2                              NaN           NaN                     NaN   \n",
       "3                              NaN           NaN                     NaN   \n",
       "4                              NaN           NaN                     NaN   \n",
       "5                              NaN           NaN                     NaN   \n",
       "6                              NaN           NaN                     NaN   \n",
       "7                              NaN           NaN                     NaN   \n",
       "8                              NaN           NaN                     NaN   \n",
       "9                              NaN           NaN                     NaN   \n",
       "10                             NaN           NaN                     NaN   \n",
       "11                             NaN           NaN                     NaN   \n",
       "12                             NaN           NaN                     NaN   \n",
       "13                             NaN           NaN                     NaN   \n",
       "14                             NaN           NaN                     NaN   \n",
       "15                             NaN           NaN                     NaN   \n",
       "16                             NaN           NaN                     NaN   \n",
       "17                             NaN           NaN                     NaN   \n",
       "18                             NaN           NaN                     NaN   \n",
       "19                             NaN           NaN                     NaN   \n",
       "20                             NaN           NaN                     NaN   \n",
       "21                             NaN           NaN                     NaN   \n",
       "22                             NaN           NaN                     NaN   \n",
       "23                             NaN           NaN                     NaN   \n",
       "24                             NaN           NaN                     NaN   \n",
       "25                             NaN           NaN                     NaN   \n",
       "26                             NaN           NaN                     NaN   \n",
       "27                             NaN           NaN                     NaN   \n",
       "28                             NaN           NaN                     NaN   \n",
       "29                             NaN           NaN                     NaN   \n",
       "30                             NaN           NaN                     NaN   \n",
       "31                             NaN           NaN                     NaN   \n",
       "32                             NaN           NaN                     NaN   \n",
       "33                             NaN           NaN                     NaN   \n",
       "34                             NaN           NaN                     NaN   \n",
       "35                             NaN           NaN                     NaN   \n",
       "36                             NaN           NaN                     NaN   \n",
       "37                             NaN           NaN                     NaN   \n",
       "38                             NaN           NaN                     NaN   \n",
       "39                             NaN           NaN                     NaN   \n",
       "40                             NaN           NaN                     NaN   \n",
       "41                             NaN           NaN                     NaN   \n",
       "42                             NaN           NaN                     NaN   \n",
       "43                             NaN      0.019125                     NaN   \n",
       "44                             NaN           NaN                     NaN   \n",
       "45                             NaN           NaN                     NaN   \n",
       "46                             NaN           NaN                     NaN   \n",
       "47                             NaN           NaN                     NaN   \n",
       "48                             NaN           NaN                     NaN   \n",
       "49                             NaN           NaN                     NaN   \n",
       "\n",
       "    Sim_to_austrian  \n",
       "0               NaN  \n",
       "1               NaN  \n",
       "2               NaN  \n",
       "3               NaN  \n",
       "4               NaN  \n",
       "5               NaN  \n",
       "6               NaN  \n",
       "7               NaN  \n",
       "8               NaN  \n",
       "9               NaN  \n",
       "10              NaN  \n",
       "11              NaN  \n",
       "12              NaN  \n",
       "13              NaN  \n",
       "14              NaN  \n",
       "15              NaN  \n",
       "16              NaN  \n",
       "17              NaN  \n",
       "18              NaN  \n",
       "19              NaN  \n",
       "20              NaN  \n",
       "21              NaN  \n",
       "22              NaN  \n",
       "23              NaN  \n",
       "24              NaN  \n",
       "25              NaN  \n",
       "26              NaN  \n",
       "27              NaN  \n",
       "28              NaN  \n",
       "29              NaN  \n",
       "30              NaN  \n",
       "31              NaN  \n",
       "32              NaN  \n",
       "33              NaN  \n",
       "34              NaN  \n",
       "35              NaN  \n",
       "36              NaN  \n",
       "37              NaN  \n",
       "38              NaN  \n",
       "39              NaN  \n",
       "40              NaN  \n",
       "41              NaN  \n",
       "42              NaN  \n",
       "43              NaN  \n",
       "44              NaN  \n",
       "45          0.24934  \n",
       "46              NaN  \n",
       "47              NaN  \n",
       "48              NaN  \n",
       "49              NaN  \n",
       "\n",
       "[50 rows x 25 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def analyze_bats_protected_class_similarities(\n",
    "    word2vec_model, \n",
    "    bats_file_path, \n",
    "    protected_class_words_list,\n",
    "    selected_target_word_index=0,\n",
    "    similarity_diff_threshold=0.1\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes similarity scores from a BATS file between target words and protected class words.\n",
    "\n",
    "    Parameters:\n",
    "        word2vec_model (gensim.models.KeyedVectors): Pretrained Word2Vec model.\n",
    "        bats_file_path (str): File path to the selected BATS file.\n",
    "        protected_class_words_list (list of str): Three words representing a protected class (e.g., national origin).\n",
    "        selected_target_word_index (int): Index of target word in each row (default 0, i.e., first word).\n",
    "        similarity_diff_threshold (float): Threshold to flag noticeable similarity differences for each reference\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with similarity scores and flags for noticeable differences for easy reference later\n",
    "    \"\"\"\n",
    "    all_bats_similarity_result_rows = []  # List to collect row similarity results\n",
    "\n",
    "    with open(bats_file_path, 'r') as bats_file_object:  # Open the BATS analogy dataset file\n",
    "        for line_index_number, line_text_content in enumerate(bats_file_object, 1):  # Enumerate lines with index\n",
    "            cleaned_word_tokens_list = line_text_content.strip().split()  # Clean and split words in the line\n",
    "            if len(cleaned_word_tokens_list) < 2:  # Ensure at least two words exist for processing\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                selected_target_word_string = cleaned_word_tokens_list[selected_target_word_index]  # Get the target word\n",
    "            except IndexError:\n",
    "                continue  # Skip lines with malformed structure\n",
    "\n",
    "            similarity_scores_row_dict = {  # Dictionary for storing similarity results for current line\n",
    "                'Line_Number': line_index_number,\n",
    "                'Target_Word': selected_target_word_string,\n",
    "                'Row_Words': cleaned_word_tokens_list\n",
    "            }\n",
    "\n",
    "            for row_word_token in cleaned_word_tokens_list:  # Compare target word to each row word\n",
    "                if row_word_token == selected_target_word_string:  # Skip self-comparison\n",
    "                    continue\n",
    "                try:\n",
    "                    similarity_score_value = word2vec_model.similarity(selected_target_word_string, row_word_token)\n",
    "                except KeyError:\n",
    "                    similarity_score_value = None  # Word missing from vocabulary\n",
    "                \n",
    "                # Save similarity with dynamically constructed column name\n",
    "                similarity_scores_row_dict[f'Sim_to_{row_word_token}'] = similarity_score_value\n",
    "\n",
    "            for protected_class_word in protected_class_words_list:  # Loop through protected class reference terms\n",
    "                try:\n",
    "                    protected_word_similarity_score = word2vec_model.similarity(selected_target_word_string, protected_class_word)\n",
    "                except KeyError:\n",
    "                    protected_word_similarity_score = None  # Handle words missing from vocab\n",
    "                \n",
    "                # Save protected class similarity with clearly labeled column\n",
    "                similarity_scores_row_dict[f'Sim_to_Protected_{protected_class_word}'] = protected_word_similarity_score\n",
    "\n",
    "            valid_protected_similarities_list = [  # Collect non-null similarity scores for comparison\n",
    "                similarity_scores_row_dict[f'Sim_to_Protected_{protected_word}']\n",
    "                for protected_word in protected_class_words_list\n",
    "                if similarity_scores_row_dict[f'Sim_to_Protected_{protected_word}'] is not None\n",
    "            ]\n",
    "\n",
    "            if len(valid_protected_similarities_list) == len(protected_class_words_list):  # Check all protected comparisons present\n",
    "                maximum_similarity_score = max(valid_protected_similarities_list)  # Find maximum similarity\n",
    "                minimum_similarity_score = min(valid_protected_similarities_list)  # Find minimum similarity\n",
    "                flag_significant_similarity_difference = (\n",
    "                    maximum_similarity_score - minimum_similarity_score >= similarity_diff_threshold\n",
    "                )  # Set flag if difference exceeds threshold\n",
    "            else:\n",
    "                flag_significant_similarity_difference = False  # Incomplete comparisons mean no valid difference check\n",
    "            \n",
    "            similarity_scores_row_dict['Noticeable_Protected_Class_Sim_Diff'] = flag_significant_similarity_difference  # Final decision flag\n",
    "            \n",
    "            all_bats_similarity_result_rows.append(similarity_scores_row_dict)  # Append result dict to master list\n",
    "\n",
    "    bats_similarity_results_dataframe = pd.DataFrame(all_bats_similarity_result_rows)  # Convert collected results into a DataFrame\n",
    "\n",
    "    return bats_similarity_results_dataframe  # Return final similarity report\n",
    "\n",
    "\n",
    "# Usage example\n",
    "bats_file_path_example = 'BATS_3.0/3_Encyclopedic_semantics/E04 [name - nationality].txt'\n",
    "protected_words_example = ['korean', 'african', 'american']\n",
    "\n",
    "bats_results_dataframe = analyze_bats_protected_class_similarities(\n",
    "    word2vec_model=newmodel,\n",
    "    bats_file_path=bats_file_path_example,\n",
    "    protected_class_words_list=protected_words_example,\n",
    "    selected_target_word_index=0,\n",
    "    similarity_diff_threshold=0.1\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"BATS National Origin Protected Class Similarities:\")\n",
    "display(bats_results_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fc57c04f-a207-4185-8ed9-bdccc00d61a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Part A Results ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>First_Pair_Word1</th>\n",
       "      <th>First_Pair_Word2</th>\n",
       "      <th>Second_Pair_Word1</th>\n",
       "      <th>Second_Pair_Word2</th>\n",
       "      <th>Similarity_First_Pair</th>\n",
       "      <th>Similarity_Second_Pair</th>\n",
       "      <th>Difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>king</td>\n",
       "      <td>throne</td>\n",
       "      <td>judge</td>\n",
       "      <td>bench</td>\n",
       "      <td>0.597070</td>\n",
       "      <td>0.302673</td>\n",
       "      <td>0.294396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>giant</td>\n",
       "      <td>dwarf</td>\n",
       "      <td>genius</td>\n",
       "      <td>imbecile</td>\n",
       "      <td>0.480748</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>college</td>\n",
       "      <td>dean</td>\n",
       "      <td>jail</td>\n",
       "      <td>warden</td>\n",
       "      <td>0.361748</td>\n",
       "      <td>0.277774</td>\n",
       "      <td>0.083974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arc</td>\n",
       "      <td>circle</td>\n",
       "      <td>line</td>\n",
       "      <td>square</td>\n",
       "      <td>0.297496</td>\n",
       "      <td>0.192632</td>\n",
       "      <td>0.104864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>French</td>\n",
       "      <td>France</td>\n",
       "      <td>Dutch</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>man</td>\n",
       "      <td>woman</td>\n",
       "      <td>king</td>\n",
       "      <td>queen</td>\n",
       "      <td>0.587694</td>\n",
       "      <td>0.568557</td>\n",
       "      <td>0.019137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>water</td>\n",
       "      <td>ice</td>\n",
       "      <td>liquid</td>\n",
       "      <td>frozen</td>\n",
       "      <td>0.325372</td>\n",
       "      <td>0.360773</td>\n",
       "      <td>0.035401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bad</td>\n",
       "      <td>good</td>\n",
       "      <td>sad</td>\n",
       "      <td>happy</td>\n",
       "      <td>0.656167</td>\n",
       "      <td>0.448851</td>\n",
       "      <td>0.207316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nurse</td>\n",
       "      <td>hospital</td>\n",
       "      <td>teacher</td>\n",
       "      <td>school</td>\n",
       "      <td>0.428714</td>\n",
       "      <td>0.532657</td>\n",
       "      <td>0.103943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>usa</td>\n",
       "      <td>pizza</td>\n",
       "      <td>japan</td>\n",
       "      <td>ramen</td>\n",
       "      <td>0.084279</td>\n",
       "      <td>0.009603</td>\n",
       "      <td>0.074676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>human</td>\n",
       "      <td>house</td>\n",
       "      <td>dog</td>\n",
       "      <td>kennel</td>\n",
       "      <td>0.022442</td>\n",
       "      <td>0.284160</td>\n",
       "      <td>0.261717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>grass</td>\n",
       "      <td>green</td>\n",
       "      <td>sky</td>\n",
       "      <td>blue</td>\n",
       "      <td>0.393804</td>\n",
       "      <td>0.443970</td>\n",
       "      <td>0.050166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>video</td>\n",
       "      <td>cassette</td>\n",
       "      <td>computer</td>\n",
       "      <td>cd</td>\n",
       "      <td>0.527228</td>\n",
       "      <td>0.236799</td>\n",
       "      <td>0.290429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>universe</td>\n",
       "      <td>planet</td>\n",
       "      <td>house</td>\n",
       "      <td>bacteria</td>\n",
       "      <td>0.434313</td>\n",
       "      <td>-0.110053</td>\n",
       "      <td>0.544366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>poverty</td>\n",
       "      <td>wealth</td>\n",
       "      <td>sickness</td>\n",
       "      <td>health</td>\n",
       "      <td>0.315178</td>\n",
       "      <td>0.195276</td>\n",
       "      <td>0.119902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   First_Pair_Word1 First_Pair_Word2 Second_Pair_Word1 Second_Pair_Word2  \\\n",
       "0              king           throne             judge             bench   \n",
       "1             giant            dwarf            genius          imbecile   \n",
       "2           college             dean              jail            warden   \n",
       "3               arc           circle              line            square   \n",
       "4            French           France             Dutch       Netherlands   \n",
       "5               man            woman              king             queen   \n",
       "6             water              ice            liquid            frozen   \n",
       "7               bad             good               sad             happy   \n",
       "8             nurse         hospital           teacher            school   \n",
       "9               usa            pizza             japan             ramen   \n",
       "10            human            house               dog            kennel   \n",
       "11            grass            green               sky              blue   \n",
       "12            video         cassette          computer                cd   \n",
       "13         universe           planet             house          bacteria   \n",
       "14          poverty           wealth          sickness            health   \n",
       "\n",
       "    Similarity_First_Pair  Similarity_Second_Pair  Difference  \n",
       "0                0.597070                0.302673    0.294396  \n",
       "1                0.480748                     NaN         NaN  \n",
       "2                0.361748                0.277774    0.083974  \n",
       "3                0.297496                0.192632    0.104864  \n",
       "4                     NaN                     NaN         NaN  \n",
       "5                0.587694                0.568557    0.019137  \n",
       "6                0.325372                0.360773    0.035401  \n",
       "7                0.656167                0.448851    0.207316  \n",
       "8                0.428714                0.532657    0.103943  \n",
       "9                0.084279                0.009603    0.074676  \n",
       "10               0.022442                0.284160    0.261717  \n",
       "11               0.393804                0.443970    0.050166  \n",
       "12               0.527228                0.236799    0.290429  \n",
       "13               0.434313               -0.110053    0.544366  \n",
       "14               0.315178                0.195276    0.119902  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Part B Results ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word_A</th>\n",
       "      <th>Word_B</th>\n",
       "      <th>Word_C</th>\n",
       "      <th>Predicted_Word</th>\n",
       "      <th>Predicted_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>king</td>\n",
       "      <td>throne</td>\n",
       "      <td>judge</td>\n",
       "      <td>prosecution</td>\n",
       "      <td>0.518646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>giant</td>\n",
       "      <td>dwarf</td>\n",
       "      <td>genius</td>\n",
       "      <td>theorist</td>\n",
       "      <td>0.428089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>college</td>\n",
       "      <td>dean</td>\n",
       "      <td>jail</td>\n",
       "      <td>peress</td>\n",
       "      <td>0.544443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arc</td>\n",
       "      <td>circle</td>\n",
       "      <td>line</td>\n",
       "      <td>lines</td>\n",
       "      <td>0.428753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>French</td>\n",
       "      <td>France</td>\n",
       "      <td>Dutch</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>man</td>\n",
       "      <td>woman</td>\n",
       "      <td>king</td>\n",
       "      <td>queen</td>\n",
       "      <td>0.553245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>water</td>\n",
       "      <td>ice</td>\n",
       "      <td>liquid</td>\n",
       "      <td>solid</td>\n",
       "      <td>0.450004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bad</td>\n",
       "      <td>good</td>\n",
       "      <td>sad</td>\n",
       "      <td>glory</td>\n",
       "      <td>0.440382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nurse</td>\n",
       "      <td>hospital</td>\n",
       "      <td>teacher</td>\n",
       "      <td>institution</td>\n",
       "      <td>0.482898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>usa</td>\n",
       "      <td>pizza</td>\n",
       "      <td>japan</td>\n",
       "      <td>dishes</td>\n",
       "      <td>0.576351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>human</td>\n",
       "      <td>house</td>\n",
       "      <td>dog</td>\n",
       "      <td>hound</td>\n",
       "      <td>0.423166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>grass</td>\n",
       "      <td>green</td>\n",
       "      <td>sky</td>\n",
       "      <td>blue</td>\n",
       "      <td>0.547864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>video</td>\n",
       "      <td>cassette</td>\n",
       "      <td>computer</td>\n",
       "      <td>peripherals</td>\n",
       "      <td>0.665451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>universe</td>\n",
       "      <td>planet</td>\n",
       "      <td>house</td>\n",
       "      <td>houses</td>\n",
       "      <td>0.426470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>poverty</td>\n",
       "      <td>wealth</td>\n",
       "      <td>sickness</td>\n",
       "      <td>impious</td>\n",
       "      <td>0.496061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Word_A    Word_B    Word_C Predicted_Word  Predicted_Score\n",
       "0       king    throne     judge    prosecution         0.518646\n",
       "1      giant     dwarf    genius       theorist         0.428089\n",
       "2    college      dean      jail         peress         0.544443\n",
       "3        arc    circle      line          lines         0.428753\n",
       "4     French    France     Dutch           None              NaN\n",
       "5        man     woman      king          queen         0.553245\n",
       "6      water       ice    liquid          solid         0.450004\n",
       "7        bad      good       sad          glory         0.440382\n",
       "8      nurse  hospital   teacher    institution         0.482898\n",
       "9        usa     pizza     japan         dishes         0.576351\n",
       "10     human     house       dog          hound         0.423166\n",
       "11     grass     green       sky           blue         0.547864\n",
       "12     video  cassette  computer    peripherals         0.665451\n",
       "13  universe    planet     house         houses         0.426470\n",
       "14   poverty    wealth  sickness        impious         0.496061"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Part C Correlation Result ===\n",
      "Pearson correlation: 0.144 (p-value: 0.623)\n"
     ]
    }
   ],
   "source": [
    "# Analogy pairs are arranged as alternating training and testing pairs\n",
    "analogy_pairs = [\n",
    "    ('king', 'throne'), ('judge', 'bench'),\n",
    "    ('giant', 'dwarf'), ('genius', 'imbecile'),\n",
    "    ('college', 'dean'), ('jail', 'warden'),\n",
    "    ('arc', 'circle'), ('line', 'square'),\n",
    "    ('French', 'France'), ('Dutch', 'Netherlands'),\n",
    "    ('man', 'woman'), ('king', 'queen'),\n",
    "    ('water', 'ice'), ('liquid', 'frozen'),\n",
    "    ('bad', 'good'), ('sad', 'happy'),\n",
    "    ('nurse', 'hospital'), ('teacher', 'school'),\n",
    "    ('usa', 'pizza'), ('japan', 'ramen'),\n",
    "    ('human', 'house'), ('dog', 'kennel'),\n",
    "    ('grass', 'green'), ('sky', 'blue'),\n",
    "    ('video', 'cassette'), ('computer', 'cd'),\n",
    "    ('universe', 'planet'), ('house', 'bacteria'),\n",
    "    ('poverty', 'wealth'), ('sickness', 'health')\n",
    "]\n",
    "\n",
    "# Separate training (odd index) from testing (even index) by slicing every other pair\n",
    "analogy_pairs_first = analogy_pairs[0::2]   # Contains primary analogy references like ('king', 'throne')\n",
    "analogy_pairs_second = analogy_pairs[1::2]  # Contains test analogy references like ('judge', 'bench')\n",
    "\n",
    "# Define function to compute pairwise cosine similarities between word pairs\n",
    "def compute_analogy_pairwise_similarities(model, pairings_one, pairings_two):\n",
    "    similarity_scores_one = []       # List to hold similarity scores for the first (training) pair\n",
    "    similarity_scores_two = []       # List to hold similarity scores for the second (test) pair\n",
    "    paired_difference_scores = []    # List to hold absolute difference between the two similarity values\n",
    "\n",
    "    # Iterate through the zipped pairs (one from each list at a time)\n",
    "    for (w1a, w1b), (w2a, w2b) in zip(pairings_one, pairings_two):\n",
    "        try:\n",
    "            sim1 = model.similarity(w1a, w1b)    # Calculate cosine similarity between first word pair\n",
    "        except KeyError:\n",
    "            sim1 = None                          # If word not found in vocab, assign None\n",
    "\n",
    "        try:\n",
    "            sim2 = model.similarity(w2a, w2b)    # Calculate cosine similarity between second word pair\n",
    "        except KeyError:\n",
    "            sim2 = None                          # If word not found, assign None\n",
    "\n",
    "        similarity_scores_one.append(sim1)       # Store first pair similarity\n",
    "        similarity_scores_two.append(sim2)       # Store second pair similarity\n",
    "\n",
    "        # Store the absolute difference if both similarities exist, else None\n",
    "        paired_difference_scores.append(\n",
    "            abs(sim1 - sim2) if sim1 is not None and sim2 is not None else None\n",
    "        )\n",
    "\n",
    "    # Return all three vectors for later tabular output and correlation analysis\n",
    "    return similarity_scores_one, similarity_scores_two, paired_difference_scores\n",
    "\n",
    "# Run the function with our Word2Vec model and analogy pairs\n",
    "part_a_scores_1, part_a_scores_2, part_a_score_diffs = compute_analogy_pairwise_similarities(\n",
    "    newmodel, analogy_pairs_first, analogy_pairs_second\n",
    ")\n",
    "\n",
    "# List of analogical triples in the form A:B :: C:?\n",
    "analogy_triples = [\n",
    "    ('king', 'throne', 'judge'),\n",
    "    ('giant', 'dwarf', 'genius'),\n",
    "    ('college', 'dean', 'jail'),\n",
    "    ('arc', 'circle', 'line'),\n",
    "    ('French', 'France', 'Dutch'),\n",
    "    ('man', 'woman', 'king'),\n",
    "    ('water', 'ice', 'liquid'),\n",
    "    ('bad', 'good', 'sad'),\n",
    "    ('nurse', 'hospital', 'teacher'),\n",
    "    ('usa', 'pizza', 'japan'),\n",
    "    ('human', 'house', 'dog'),\n",
    "    ('grass', 'green', 'sky'),\n",
    "    ('video', 'cassette', 'computer'),\n",
    "    ('universe', 'planet', 'house'),\n",
    "    ('poverty', 'wealth', 'sickness')\n",
    "]\n",
    "\n",
    "predicted_words = []   # Store predicted word for each triple analogy\n",
    "predicted_scores = []  # Store associated similarity score for each prediction\n",
    "\n",
    "# Loop through each triple analogy and compute the vector-based analogy prediction\n",
    "for a, b, c in analogy_triples:\n",
    "    try:\n",
    "        # Use vector algebra: C + B - A should point to the predicted word\n",
    "        result = newmodel.most_similar(positive=[c, b], negative=[a], topn=1)[0]\n",
    "        predicted_words.append(result[0])       # Append most similar word to list\n",
    "        predicted_scores.append(result[1])      # Append similarity score to list\n",
    "    except KeyError:\n",
    "        predicted_words.append(None)            # Append None if any word missing from vocab\n",
    "        predicted_scores.append(None)\n",
    "\n",
    "# Prepare for correlation by filtering out any None values\n",
    "valid_indices = [i for i, (s1, s2) in enumerate(zip(part_a_scores_1, predicted_scores))\n",
    "                 if s1 is not None and s2 is not None]  # Only keep index if both scores exist\n",
    "\n",
    "manual_scores_valid = [part_a_scores_1[i] for i in valid_indices]   # Keep only valid manual scores\n",
    "model_scores_valid = [predicted_scores[i] for i in valid_indices]  # Keep only valid model scores\n",
    "\n",
    "# Calculate Pearson correlation between manually computed similarities and model's predicted analogies\n",
    "correlation_result = pearsonr(manual_scores_valid, model_scores_valid)  # Returns (correlation, p-value)\n",
    "\n",
    "# Construct DataFrame for Part A with pairwise similarities and differences\n",
    "part_a_df = pd.DataFrame({\n",
    "    'First_Pair_Word1': [x[0] for x in analogy_pairs_first],\n",
    "    'First_Pair_Word2': [x[1] for x in analogy_pairs_first],\n",
    "    'Second_Pair_Word1': [x[0] for x in analogy_pairs_second],\n",
    "    'Second_Pair_Word2': [x[1] for x in analogy_pairs_second],\n",
    "    'Similarity_First_Pair': part_a_scores_1,\n",
    "    'Similarity_Second_Pair': part_a_scores_2,\n",
    "    'Difference': part_a_score_diffs\n",
    "})\n",
    "\n",
    "part_b_df = pd.DataFrame({\n",
    "    'Word_A': [a for a, b, c in analogy_triples],\n",
    "    'Word_B': [b for a, b, c in analogy_triples],\n",
    "    'Word_C': [c for a, b, c in analogy_triples],\n",
    "    'Predicted_Word': predicted_words,\n",
    "    'Predicted_Score': predicted_scores\n",
    "})\n",
    "\n",
    "print(\"=== Part A Results: manual imput and comparison ===\")\n",
    "display(part_a_df)\n",
    "\n",
    "print(\"=== Part B Results: predicted by algorithmic cosine similarity ===\")\n",
    "display(part_b_df)\n",
    "\n",
    "print(\"=== Part C Correlation Result ===\")\n",
    "print(f\"Pearson correlation: {correlation_result[0]:.3f} (p-value: {correlation_result[1]:.3g})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c5d42081-4698-4f23-8d95-18dd22b3ef2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age  Group   Frequency Counts:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1p/55zf00x17kv1bw300f79xsnr0000gn/T/ipykernel_83428/1392412666.py:56: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  age_gender_race_cross_counts_table = demographic_metadata_dataframe_combined.groupby(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "age_group\n",
       "0â€“20      4267\n",
       "21â€“40     2533\n",
       "41â€“60     1665\n",
       "61â€“80      967\n",
       "81â€“116     346\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gender   Distribution Counts:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "gender\n",
       "female    5406\n",
       "male      4372\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Race   Category Counts:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "race\n",
       "White     5265\n",
       "Asian     1553\n",
       "Indian    1452\n",
       "Other     1103\n",
       "Black      405\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined  Distribution Table (age  group x gender x race):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age_group</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0â€“20</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0â€“20</td>\n",
       "      <td>female</td>\n",
       "      <td>Black</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0â€“20</td>\n",
       "      <td>female</td>\n",
       "      <td>Indian</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0â€“20</td>\n",
       "      <td>female</td>\n",
       "      <td>Other</td>\n",
       "      <td>361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0â€“20</td>\n",
       "      <td>female</td>\n",
       "      <td>White</td>\n",
       "      <td>1036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0â€“20</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0â€“20</td>\n",
       "      <td>male</td>\n",
       "      <td>Black</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0â€“20</td>\n",
       "      <td>male</td>\n",
       "      <td>Indian</td>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0â€“20</td>\n",
       "      <td>male</td>\n",
       "      <td>Other</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0â€“20</td>\n",
       "      <td>male</td>\n",
       "      <td>White</td>\n",
       "      <td>895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>21â€“40</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>21â€“40</td>\n",
       "      <td>female</td>\n",
       "      <td>Black</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>21â€“40</td>\n",
       "      <td>female</td>\n",
       "      <td>Indian</td>\n",
       "      <td>448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>21â€“40</td>\n",
       "      <td>female</td>\n",
       "      <td>Other</td>\n",
       "      <td>267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>21â€“40</td>\n",
       "      <td>female</td>\n",
       "      <td>White</td>\n",
       "      <td>623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>21â€“40</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>21â€“40</td>\n",
       "      <td>male</td>\n",
       "      <td>Black</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>21â€“40</td>\n",
       "      <td>male</td>\n",
       "      <td>Indian</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>21â€“40</td>\n",
       "      <td>male</td>\n",
       "      <td>Other</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>21â€“40</td>\n",
       "      <td>male</td>\n",
       "      <td>White</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>41â€“60</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>41â€“60</td>\n",
       "      <td>female</td>\n",
       "      <td>Black</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>41â€“60</td>\n",
       "      <td>female</td>\n",
       "      <td>Indian</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>41â€“60</td>\n",
       "      <td>female</td>\n",
       "      <td>Other</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>41â€“60</td>\n",
       "      <td>female</td>\n",
       "      <td>White</td>\n",
       "      <td>579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>41â€“60</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>41â€“60</td>\n",
       "      <td>male</td>\n",
       "      <td>Black</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>41â€“60</td>\n",
       "      <td>male</td>\n",
       "      <td>Indian</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>41â€“60</td>\n",
       "      <td>male</td>\n",
       "      <td>Other</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>41â€“60</td>\n",
       "      <td>male</td>\n",
       "      <td>White</td>\n",
       "      <td>673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>61â€“80</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>61â€“80</td>\n",
       "      <td>female</td>\n",
       "      <td>Black</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>61â€“80</td>\n",
       "      <td>female</td>\n",
       "      <td>Indian</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>61â€“80</td>\n",
       "      <td>female</td>\n",
       "      <td>Other</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>61â€“80</td>\n",
       "      <td>female</td>\n",
       "      <td>White</td>\n",
       "      <td>405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>61â€“80</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>61â€“80</td>\n",
       "      <td>male</td>\n",
       "      <td>Black</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>61â€“80</td>\n",
       "      <td>male</td>\n",
       "      <td>Indian</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>61â€“80</td>\n",
       "      <td>male</td>\n",
       "      <td>Other</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>61â€“80</td>\n",
       "      <td>male</td>\n",
       "      <td>White</td>\n",
       "      <td>388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>81â€“116</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>81â€“116</td>\n",
       "      <td>female</td>\n",
       "      <td>Black</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>81â€“116</td>\n",
       "      <td>female</td>\n",
       "      <td>Indian</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>81â€“116</td>\n",
       "      <td>female</td>\n",
       "      <td>Other</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>81â€“116</td>\n",
       "      <td>female</td>\n",
       "      <td>White</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>81â€“116</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>81â€“116</td>\n",
       "      <td>male</td>\n",
       "      <td>Black</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>81â€“116</td>\n",
       "      <td>male</td>\n",
       "      <td>Indian</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>81â€“116</td>\n",
       "      <td>male</td>\n",
       "      <td>Other</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>81â€“116</td>\n",
       "      <td>male</td>\n",
       "      <td>White</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age_group  gender    race  count\n",
       "0       0â€“20  female   Asian    496\n",
       "1       0â€“20  female   Black     92\n",
       "2       0â€“20  female  Indian    341\n",
       "3       0â€“20  female   Other    361\n",
       "4       0â€“20  female   White   1036\n",
       "5       0â€“20    male   Asian    521\n",
       "6       0â€“20    male   Black     68\n",
       "7       0â€“20    male  Indian    266\n",
       "8       0â€“20    male   Other    191\n",
       "9       0â€“20    male   White    895\n",
       "10     21â€“40  female   Asian    239\n",
       "11     21â€“40  female   Black     55\n",
       "12     21â€“40  female  Indian    448\n",
       "13     21â€“40  female   Other    267\n",
       "14     21â€“40  female   White    623\n",
       "15     21â€“40    male   Asian    110\n",
       "16     21â€“40    male   Black     45\n",
       "17     21â€“40    male  Indian    150\n",
       "18     21â€“40    male   Other    185\n",
       "19     21â€“40    male   White    411\n",
       "20     41â€“60  female   Asian     30\n",
       "21     41â€“60  female   Black     39\n",
       "22     41â€“60  female  Indian     88\n",
       "23     41â€“60  female   Other     15\n",
       "24     41â€“60  female   White    579\n",
       "25     41â€“60    male   Asian     58\n",
       "26     41â€“60    male   Black     36\n",
       "27     41â€“60    male  Indian     74\n",
       "28     41â€“60    male   Other     73\n",
       "29     41â€“60    male   White    673\n",
       "30     61â€“80  female   Asian     19\n",
       "31     61â€“80  female   Black     10\n",
       "32     61â€“80  female  Indian     28\n",
       "33     61â€“80  female   Other      3\n",
       "34     61â€“80  female   White    405\n",
       "35     61â€“80    male   Asian     28\n",
       "36     61â€“80    male   Black     45\n",
       "37     61â€“80    male  Indian     35\n",
       "38     61â€“80    male   Other      6\n",
       "39     61â€“80    male   White    388\n",
       "40    81â€“116  female   Asian     36\n",
       "41    81â€“116  female   Black      8\n",
       "42    81â€“116  female  Indian     15\n",
       "43    81â€“116  female   Other      2\n",
       "44    81â€“116  female   White    171\n",
       "45    81â€“116    male   Asian     16\n",
       "46    81â€“116    male   Black      7\n",
       "47    81â€“116    male  Indian      7\n",
       "48    81â€“116    male   Other      0\n",
       "49    81â€“116    male   White     84"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the path   to  where the cropped images are loacted\n",
    "extracted_images_directory_path = 'crop_part1'  # Settt path  to image folder containing the UTK face datassett\n",
    "\n",
    "# Create empty  lists  to store each piece of metadata  from file names\n",
    "age_value_list_extracted = []  # Store the extracted agee   valus\n",
    "gender_code_list_extracted = []  # Store gender codes (0 or 1) from  file naming conventionn\n",
    "race_code_list_extracted = []  # Store racial code (0 to 4)  accorrding to UTK docs\n",
    "\n",
    "# Loop through  filenames to parse out the age, gender, and race\n",
    "for individual_image_filename in os.listdir(extracted_images_directory_path):  # Iterrate over each file in folder\n",
    "    if individual_image_filename.endswith('.jpg'):  # Ensurre only .jpg files processed\n",
    "        try:\n",
    "            extracted_age, extracted_gender, extracted_race = map(int, individual_image_filename.split('_')[:3])  # Get first 3 vals\n",
    "            age_value_list_extracted.append(extracted_age)  # Append to agee listt\n",
    "            gender_code_list_extracted.append(extracted_gender)  # Append gender codee (0/1)\n",
    "            race_code_list_extracted.append(extracted_race)  # Append race code from filename  split\n",
    "        except ValueError:\n",
    "            continue  # skip the file if it cannott  be parsed properly\n",
    "\n",
    "# Assemble the  lists into a DataFrame for downstream processing\n",
    "demographic_metadata_dataframe_combined = pd.DataFrame({  # Combine the extracted metadata into pandas DataFrame\n",
    "    'age': age_value_list_extracted,  #   Add age column\n",
    "    'gender': gender_code_list_extracted,  # Add gender column\n",
    "    'race': race_code_list_extracted  # Add race colummn\n",
    "})\n",
    "\n",
    "# Mapping dictionaries to convert coded values to human-readable strings\n",
    "gender_label_mapping_dictionary = {0: 'male', 1: 'female'}  # 0 means male,  1 means fmeale\n",
    "race_label_mapping_dictionary = {\n",
    "    0: 'White',     # Code 0  -> White\n",
    "    1: 'Black',     # Code 1  -> Black\n",
    "    2: 'Asian',     # Code 2  -> Asian\n",
    "    3: 'Indian',    # Code 3  -> Indiian\n",
    "    4: 'Other'      # Code 4  -> Other (like Middle Eastern, Latino, etc.)\n",
    "}\n",
    "\n",
    "# Apply the decoding dictionaries to create readable colmns\n",
    "demographic_metadata_dataframe_combined['gender'] = demographic_metadata_dataframe_combined['gender'].map(gender_label_mapping_dictionary)  # Map gender values\n",
    "demographic_metadata_dataframe_combined['race'] = demographic_metadata_dataframe_combined['race'].map(race_label_mapping_dictionary)  # Map racee values\n",
    "\n",
    "# Create age group categorizations based on bins\n",
    "age_range_bins_defined = [0, 20, 40, 60, 80, 116]  # Agee intervals from isntruction guidelines\n",
    "age_range_labels_assigned = ['0â€“20', '21â€“40', '41â€“60', '61â€“80', '81â€“116']  # easy labels for quick reading \n",
    "demographic_metadata_dataframe_combined['age_group'] = pd.cut(\n",
    "    demographic_metadata_dataframe_combined['age'], \n",
    "    bins=age_range_bins_defined, \n",
    "    labels=age_range_labels_assigned,\n",
    "    right=True, include_lowest=True)  # Apply binning to create new col for age buckets\n",
    "\n",
    "# Count frequenceis across age  groupings\n",
    "age_group_frequency_counts = demographic_metadata_dataframe_combined['age_group'].value_counts().sort_index()  # Count by age_group\n",
    "gender_category_frequency_counts = demographic_metadata_dataframe_combined['gender'].value_counts()  # Count by genderr\n",
    "race_category_frequency_counts = demographic_metadata_dataframe_combined['race'].value_counts()  # Count by race type\n",
    "\n",
    "# Grouped combination table for age group + gender + race breakdown\n",
    "age_gender_race_cross_counts_table = demographic_metadata_dataframe_combined.groupby(\n",
    "    ['age_group', 'gender', 'race']\n",
    ").size().reset_index(name='count')  # Create cross table of age,  gender, and race\n",
    "\n",
    "# Print or  display final output tables\n",
    "print(\"Age  Group   Frequency Counts:\")\n",
    "display(age_group_frequency_counts)  # Shows grouped freq  for age  groups\n",
    "\n",
    "print(\"\\nGender   Distribution Counts:\")\n",
    "display(gender_category_frequency_counts)  # Output gender  tallys\n",
    "\n",
    "print(\"\\nRace   Category Counts:\")\n",
    "display(race_category_frequency_counts)  # Show the  race label freqs\n",
    "\n",
    "print(\"\\nCombined  Distribution Table (age  group x gender x race):\")\n",
    "display(age_gender_race_cross_counts_table)  # Final combined  breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaf9819-e030-475f-84cb-53741ac8f0c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
