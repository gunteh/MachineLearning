{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153a9720-1ff6-4ff5-9609-c4519331b430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn: model selection, modeling, preprocessing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Scikit-learn: metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    make_scorer,\n",
    "    precision_recall_curve\n",
    ")\n",
    "\n",
    "# Oversampling Method (Like SMOTE)\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# AIF360: fairness-aware tools\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.metrics import ClassificationMetric\n",
    "\n",
    "# Outlier detection\n",
    "from scipy import stats\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f95f8e-2921-40ee-8063-efa8d2864995",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea75d0a-bf4b-44c1-ad39-f9b8eb2c9f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"german.data\", sep=r'\\s+', header=None) # Loading the dataset\n",
    "df2 = pd.read_csv(\"german.data-numeric\", sep=r'\\s+', header=None) # Loading the dataset\n",
    "# dimension investigation of head\n",
    "print(df.head())\n",
    "print(df.shape)\n",
    "for val in sorted(df[8].unique()):\n",
    "    print(f\"Value: {val}\")\n",
    "for val in sorted(df2[8].unique()):\n",
    "    print(f\"Value: {val}\")\n",
    "print(df2.head())\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ba8560-15f7-4c54-b6a1-6398de79b019",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum() # Doulbe checking for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c777f84b-30b4-4ec0-b454-393764d2598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes # Checking data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804cc5bb-0988-4392-9d10-98cddf85a611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical columns as needed (optional, for modeling purposes later)\n",
    "categorical_cols = df.select_dtypes(include='object').columns\n",
    "for col in categorical_cols:\n",
    "    df[col] = df[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcc2f2b-26fc-4587-8efb-ccfd5a027100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of numeric variables\n",
    "print(df.describe())\n",
    "\n",
    "# Summary of categorical variables\n",
    "# print(df[categorical_cols].describe())\n",
    "\n",
    "# Summary of numeric variables\n",
    "print(df2.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec66b4a7-f30d-4e8a-87da-d148f8d3a640",
   "metadata": {},
   "source": [
    "# Data Treatment German Credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d8b7a8-ddb5-4437-9a54-ba97e0bf88e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns for df for readabiliyt\n",
    "column_naming = [\n",
    "    'checking_account', 'duration', 'credit_history', 'purpose', 'credit_amount',\n",
    "    'savings_account', 'employment', 'installment_rate', 'personal_status_sex',\n",
    "    'other_debtors', 'residence_since', 'property', 'age', 'other_installment_plans',\n",
    "    'housing', 'existing_credits', 'job', 'num_dependents', 'telephone',\n",
    "    'foreign_worker', 'target'\n",
    "]\n",
    "df.columns = column_naming\n",
    "#renaming the encoded df2 (looks like it has extra columns)\n",
    "df2_main_encodings = df2.iloc[:, :20].copy()\n",
    "df2_main_encodings.columns = [f\"{col}_encoded\" for col in column_naming[:-1]] \n",
    "\n",
    "# Add target c olumn back (assuming it's the 24th column due to similarity to the df's 'target' column)\n",
    "df2_main_encodings['target_encoded'] = df2[24] # 1 = \"good\" , 2 = \"bad\" for target\n",
    "\n",
    "# Merge side-by-side\n",
    "df_merged = pd.concat([df, df2_main_encodings], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f12213-4317-49bb-93d8-d7f642fdef6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df2_main_encodings.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d972188a-3846-4c4f-967a-ed1f9950341a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_merged.head())\n",
    "for val in sorted(df_merged['personal_status_sex'].unique()):\n",
    "    print(f\"Value: {val}\")\n",
    "\n",
    "# Provided human-readable mapping\n",
    "label_dict = {\n",
    "    'A91': 2,\n",
    "    'A92': 3,\n",
    "    'A93': 4,\n",
    "    'A94': 5,\n",
    "    'A95': 6\n",
    "}\n",
    "\n",
    "# Map the labels to the column\n",
    "df_merged['personal_status_sex_label'] = df_merged['personal_status_sex'].map(label_dict)\n",
    "\n",
    "le = LabelEncoder()\n",
    "df_merged['personal_status_sex_encoded'] = le.fit_transform(df_merged['personal_status_sex_label'])\n",
    "\n",
    "# Get unique value pairs between original and encoded columns\n",
    "unique_pairs = df_merged[['personal_status_sex', 'personal_status_sex_encoded']].drop_duplicates()\n",
    "\n",
    "# Sort the result for easier viewing (optional)\n",
    "unique_pairs = unique_pairs.sort_values(by='personal_status_sex_encoded').reset_index(drop=True)\n",
    "\n",
    "# Display the result\n",
    "print(unique_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0b94ff-cdd6-43e5-9e24-6b036290442e",
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_mapping_german_credit = {\n",
    "    'checking_account': {\n",
    "        'A11': '... < 0 DM',\n",
    "        'A12': '0 <= ... < 200 DM',\n",
    "        'A13': '... >= 200 DM / salary assignments for at least 1 year',\n",
    "        'A14': 'no checking account'\n",
    "    },\n",
    "    'credit_history': {\n",
    "        'A30': 'no credits taken/ all credits paid back duly',\n",
    "        'A31': 'all credits at this bank paid back duly',\n",
    "        'A32': 'existing credits paid back duly till now',\n",
    "        'A33': 'delay in paying off in the past',\n",
    "        'A34': 'critical account/ other credits existing (not at this bank)'\n",
    "    },\n",
    "    'purpose': {\n",
    "        'A40': 'car (new)', 'A41': 'car (used)', 'A42': 'furniture/equipment',\n",
    "        'A43': 'radio/television', 'A44': 'domestic appliances', 'A45': 'repairs',\n",
    "        'A46': 'education', 'A47': '(vacation - does not exist?)',\n",
    "        'A48': 'retraining', 'A49': 'business', 'A410': 'others'\n",
    "    },\n",
    "    'savings_account': {\n",
    "        'A61': '... < 100 DM', 'A62': '100 <= ... < 500 DM',\n",
    "        'A63': '500 <= ... < 1000 DM', 'A64': '... >= 1000 DM',\n",
    "        'A65': 'unknown/ no savings account'\n",
    "    },\n",
    "    'employment': {\n",
    "        'A71': 'unemployed', 'A72': '... < 1 year',\n",
    "        'A73': '1 <= ... < 4 years', 'A74': '4 <= ... < 7 years',\n",
    "        'A75': '... >= 7 years'\n",
    "    },\n",
    "    'personal_status_sex': {\n",
    "        'A91': 'male : divorced/separated',\n",
    "        'A92': 'female : divorced/separated/married',\n",
    "        'A93': 'male : single',\n",
    "        'A94': 'male : married/widowed',\n",
    "        'A95': 'female : single'\n",
    "    },\n",
    "    'other_debtors': {\n",
    "        'A101': 'none', 'A102': 'co-applicant', 'A103': 'guarantor'\n",
    "    },\n",
    "    'property': {\n",
    "        'A121': 'real estate',\n",
    "        'A122': 'building society savings agreement/ life insurance',\n",
    "        'A123': 'car or other, not in attribute 6',\n",
    "        'A124': 'unknown / no property'\n",
    "    },\n",
    "    'other_installment_plans': {\n",
    "        'A141': 'bank', 'A142': 'stores', 'A143': 'none'\n",
    "    },\n",
    "    'housing': {\n",
    "        'A151': 'rent', 'A152': 'own', 'A153': 'for free'\n",
    "    },\n",
    "    'job': {\n",
    "        'A171': 'unemployed/ unskilled - non-resident',\n",
    "        'A172': 'unskilled - resident',\n",
    "        'A173': 'skilled employee / official',\n",
    "        'A174': 'management/ self-employed/ highly qualified employee/ officer'\n",
    "    },\n",
    "    'telephone': {\n",
    "        'A191': 'none', 'A192': 'yes, registered under the customers name'\n",
    "    },\n",
    "    'foreign_worker': {\n",
    "        'A201': 'yes', 'A202': 'no'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850b0a9b-eb97-4547-98da-85ab2e0f309f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoders = {}  # Dictionary to store LabelEncoder instances for each categorical column\n",
    "\n",
    "for col in categorical_cols:  # Loop through each categorical column\n",
    "    le = LabelEncoder()  # Create a new LabelEncoder instance\n",
    "    df_merged[col + '_encoded'] = le.fit_transform(df_merged[col])  # Encode the column and add it to the DataFrame with a new name\n",
    "    label_encoders[col] = le  # Store the encoder for potential reverse lookup later\n",
    "\n",
    "encoded_to_original_mapping = {}  # Dictionary to store mappings from encoded values back to original values\n",
    "\n",
    "for col in df_merged.columns:  # Loop through all columns in the DataFrame\n",
    "    if col.endswith('_encoded'):  # Only process columns that were encoded\n",
    "        original_col = col.replace('_encoded', '')  # Derive the original column name\n",
    "        mapping_df = df_merged[[original_col, col]].drop_duplicates()  # Create a DataFrame of unique original-encoded pairs\n",
    "        \n",
    "        mapping_dict = defaultdict(set)  # Use a defaultdict to collect original values for each encoded value\n",
    "        for _, row in mapping_df.iterrows():  # Iterate through each row of the mapping DataFrame\n",
    "            mapping_dict[row[col]].add(row[original_col])  # Add the original value to the set for the encoded key\n",
    "        \n",
    "        encoded_to_original_mapping[col] = dict(mapping_dict)  # Convert defaultdict to regular dict and store in the mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5ee859-b4f5-4e55-95f1-f8b510211bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "protected_attributes = [\n",
    "    'age_encoded',                # Age\n",
    "    'personal_status_sex_encoded',  # Gender/Marital status\n",
    "    'foreign_worker_encoded',    # Nationality/Immigration status\n",
    "    'telephone_encoded',         # Proxy for access/stability\n",
    "    'job_encoded'                # Employment status\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f63910-d6de-4cb4-b424-63436c7bbe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target column and feature columns\n",
    "target_column_good_bad = 'target_encoded'\n",
    "encoded_features_columns = [col for col in df2_main_encodings.columns \n",
    "                            if col.endswith('_encoded') and col != target_column_good_bad]\n",
    "\n",
    "# Apply one-hot encoding only to categorical features (i.e., those with object or int representing discrete categories)\n",
    "categorical_cols = df2_main_encodings[encoded_features_columns].select_dtypes(include=['object', 'category', 'int']).columns.tolist()\n",
    "\n",
    "# Perform one-hot encoding\n",
    "df2_one_hot = pd.get_dummies(df2_main_encodings, columns=categorical_cols, prefix=categorical_cols)\n",
    "\n",
    "# Extract features and target\n",
    "X = df2_one_hot.drop(columns=[target_column_good_bad])\n",
    "y = df2_one_hot[target_column_good_bad]\n",
    "\n",
    "# Split dataset into training and testing (50% / 50%) <--per original instructions\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.5, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_test_copy = X_test.copy()\n",
    "\n",
    "# remove protected classes from training and testing data\n",
    "X_train = X_train.drop(columns=[\n",
    "    col for col in X_train.columns\n",
    "    if any(col.startswith(prefix) for prefix in protected_attributes)\n",
    "])\n",
    "\n",
    "X_test = X_test.drop(columns=[\n",
    "    col for col in X_test.columns\n",
    "    if any(col.startswith(prefix) for prefix in protected_attributes)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e41e51-d07f-4275-bb25-f01a0b981b33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(X.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1a3045-24c4-49c3-889b-3630a9866b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping feattures with zero correlation to logistic model\n",
    "# step was done later in the code but I'm removing it here so I don't have to code a \"re-run\" of the model itself\n",
    "\n",
    "drop_features_zero_correlation = [\n",
    "    'duration_encoded_5',\n",
    "    'duration_encoded_13',\n",
    "    'duration_encoded_16',\n",
    "    'duration_encoded_40',\n",
    "    'duration_encoded_54',\n",
    "    'duration_encoded_72',\n",
    "    'purpose_encoded_2',\n",
    "    'purpose_encoded_54',\n",
    "    'purpose_encoded_56',\n",
    "    'purpose_encoded_82',\n",
    "    'purpose_encoded_85',\n",
    "    'purpose_encoded_91',\n",
    "    'purpose_encoded_94',\n",
    "    'purpose_encoded_100',\n",
    "    'purpose_encoded_101',\n",
    "    'purpose_encoded_102',\n",
    "    'purpose_encoded_105',\n",
    "    'purpose_encoded_106',\n",
    "    'purpose_encoded_109',\n",
    "    'purpose_encoded_110',\n",
    "    'purpose_encoded_113',\n",
    "    'purpose_encoded_118',\n",
    "    'purpose_encoded_119',\n",
    "    'purpose_encoded_124',\n",
    "    'purpose_encoded_126',\n",
    "    'purpose_encoded_127',\n",
    "    'purpose_encoded_130',\n",
    "    'purpose_encoded_138',\n",
    "    'purpose_encoded_140',\n",
    "    'purpose_encoded_143',\n",
    "    'purpose_encoded_144',\n",
    "    'purpose_encoded_146',\n",
    "    'purpose_encoded_148',\n",
    "    'purpose_encoded_149',\n",
    "    'purpose_encoded_157',\n",
    "    'purpose_encoded_159',\n",
    "    'purpose_encoded_184',\n",
    "    'other_debtors_encoded_68'\n",
    "]\n",
    "\n",
    "X_train = X_train.drop(columns=drop_features_zero_correlation)\n",
    "\n",
    "X_test = X_test.drop(columns=drop_features_zero_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f5b94f-893a-4c21-a428-330cfa4203e3",
   "metadata": {},
   "source": [
    "# Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94882e33-6a42-44b5-a51c-547e80bcb178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipelining that scales features before applying logistic regression\n",
    "clf = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('logreg', LogisticRegression(\n",
    "        solver='liblinear', \n",
    "        random_state=42, \n",
    "        max_iter=1000,\n",
    "        class_weight={1: 1.0, 2: 10.0}\n",
    "    ))\n",
    "])\n",
    "\n",
    "#Randmon Oversampling\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# Fit the model\n",
    "clf.fit(X_resampled, y_resampled) # clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ee196a-a044-411d-830a-541a915adc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = clf.predict_proba(X_test)  # Returns the values of probabilities, on a scale of 0.00-1.00\n",
    "creditworthiness_score = 100 * probs[:, 1] # creates a scoring of 0-100\n",
    "\n",
    "for prob, score in zip(probs[:5, 1], creditworthiness_score[:5]):\n",
    "    print(f\"Predicted P(y=1): {prob:.2f}, Score: {score:.0f}\")\n",
    "\n",
    "logreg_model = clf.named_steps['logreg'] # Get the actual LogisticRegression instance \n",
    "coefs = logreg_model.coef_[0] \n",
    "\n",
    "for feature, weight in sorted(zip(X_train.columns, coefs), key=lambda x: abs(x[1]), reverse=True): \n",
    "        print(f\"{feature}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adb45d6-3ada-48e3-b5c8-b45133953c08",
   "metadata": {},
   "source": [
    "# Detour: Analysis of impact of model, refining, and revamping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bccb7a6-c484-4ebf-8b6b-90b1e0e25a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIAL THRESHOLD ANALYSIS DONT REVISE FOR REPORT GENERATION\n",
    "\n",
    "y_probs = clf.predict_proba(X_test)[:, 1]\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test, y_probs, pos_label=2)\n",
    "\n",
    "# Calculate F1 scores for each threshold\n",
    "f1s = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)  # add small value to avoid divide-by-zero\n",
    "\n",
    "# Plot Precision-Recall Curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(recalls, precisions, label='PR Curve', color='blue')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Plot F1 Score vs Threshold\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(thresholds, f1s[:-1], color='green')  # thresholds is 1 less than precision/recall length\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Score vs Threshold')\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543cb9cb-78d3-4dcf-876f-9ce84c807198",
   "metadata": {},
   "source": [
    "# Model Translation into scorecard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95832d1b-f8a3-440e-bb3d-e07596b043ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_logistic_regression_to_scorecard(\n",
    "    trained_logistic_regression_model,\n",
    "    feature_column_names,\n",
    "    minimum_credit_score=0,\n",
    "    maximum_credit_score=100\n",
    "):\n",
    "    \"\"\"\n",
    "    Convert a trained logistic regression model into a scorecard that maps\n",
    "    feature weights to interpretable credit scores.\n",
    "    \"\"\"\n",
    "    model_coefficients = trained_logistic_regression_model.coef_[0]  # Coefficients for each feature\n",
    "    model_intercept = trained_logistic_regression_model.intercept_[0]  # Intercept term\n",
    "\n",
    "    scorecard_entries = []  # List to hold scorecard tuples\n",
    "\n",
    "    # Calculate scaling factor to map coefficients to score range\n",
    "    max_absolute_coefficient = max(abs(model_coefficients).max(), 1e-6)  # Avoid divide-by-zero\n",
    "    coefficient_scaling_factor = (maximum_credit_score - minimum_credit_score) / (2 * max_absolute_coefficient)\n",
    "\n",
    "    # Convert intercept to bias score\n",
    "    intercept_bias_score = model_intercept * coefficient_scaling_factor + minimum_credit_score\n",
    "\n",
    "    # Add intercept to scorecard\n",
    "    scorecard_entries.append((\"Intercept (bias)\", intercept_bias_score))\n",
    "\n",
    "    # Convert each feature coefficient to a score contribution\n",
    "    for feature_name, feature_coefficient in zip(feature_column_names, model_coefficients):\n",
    "        feature_score_contribution = feature_coefficient * coefficient_scaling_factor\n",
    "        scorecard_entries.append((feature_name, feature_score_contribution))\n",
    "\n",
    "    return scorecard_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481c4a14-ff57-4af9-86fd-c1cf27ba7fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_model = clf.named_steps['logreg']\n",
    "\n",
    "scorecard_rules = convert_logistic_regression_to_scorecard(\n",
    "    trained_logistic_model=logreg_model,\n",
    "    input_feature_names=X_train.columns,\n",
    "    base_credit_score=0,\n",
    "    high_risk_credit_score=100\n",
    ")\n",
    "\n",
    "for rule_description, assigned_score in scorecard_rules:\n",
    "    print(f\"If {rule_description} => Assign Score: {assigned_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25856a2b-13e2-430b-83e9-6e07e25d43d6",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b760f69-a110-4ca7-b007-592b8bdca543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting probabilities (for class 2, i.e., \"bad credit\" score)\n",
    "y_prob = clf.predict_proba(X_test)[:, 1]\n",
    "optimal_threshold = 0.3 # Custom threshold as default is 0.5, or sweep from 0.06 to 0.10\n",
    "y_pred = (y_prob > optimal_threshold).astype(int) + 1\n",
    "\n",
    "# Convert probabilities to creditworthiness scores (0–100 scale)\n",
    "test_scores = y_prob * 100\n",
    "\n",
    "# Accuracy \"score\"\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# F1 Score (us0e average='binary' if binary classification, else 'macro' for overall)\n",
    "f1 = f1_score(y_test, y_pred, average='macro')  \n",
    "\n",
    "# Confusion Matrix: [[TN, FP], [FN, TP]]\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Extractingg false positives and false negatives\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "# Full classification report\n",
    "report = classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bd25b3-1a52-429b-af28-f403b9575986",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy Score: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(\"Confusion Matrix Results:\")\n",
    "print(conf_matrix)\n",
    "print(f\"False Positives (defined as bad credit when they are actually good credit): {fp}\")\n",
    "print(f\"False Negatives (defined as good credit when they're actually bad credit): {fn}\")\n",
    "print(\"\\nFull Classification Report Details:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba15845-833b-4fa1-93e5-f73f2a31d99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique predictions:\", np.unique(y_pred))  # Display the unique predicted class labels\n",
    "\n",
    "# Create a copy of X_test to reattach protected attributes for subgroup analysis\n",
    "X_test_with_protected_attributes = X_test.copy()\n",
    "\n",
    "# Extract protected attributes from the original encoded DataFrame using the test set index\n",
    "protected_attributes_subset = df2_main_encodings.loc[X_test.index, protected_attributes]\n",
    "\n",
    "# Join the protected attributes back into the copied test set\n",
    "X_test_with_protected_attributes = X_test_with_protected_attributes.join(protected_attributes_subset)\n",
    "\n",
    "# Convert predictions to a Series with the same index as X_test for alignment\n",
    "y_pred_series_indexed = pd.Series(y_pred, index=X_test.index)\n",
    "\n",
    "# Loop through each protected attribute for subgroup performance breakdown\n",
    "for protected_attribute in protected_attributes:\n",
    "    print(f\"\\n=== Breakdown for {protected_attribute} ===\")\n",
    "\n",
    "    # Loop through each unique group within the protected attribute\n",
    "    for group_value in sorted(X_test_with_protected_attributes[protected_attribute].unique()):\n",
    "        group_filter_mask = X_test_with_protected_attributes[protected_attribute] == group_value  # Boolean mask for group\n",
    "\n",
    "        true_labels_for_group = y_test[group_filter_mask]  # True labels for the group\n",
    "        predicted_labels_for_group = y_pred_series_indexed[group_filter_mask]  # Predicted labels for the group\n",
    "\n",
    "        unique_class_labels = set(true_labels_for_group) | set(predicted_labels_for_group)  # Union of true and predicted classes\n",
    "\n",
    "        # Handle edge case: if only one class is present, F1 score is undefined\n",
    "        if len(unique_class_labels) < 2:\n",
    "            f1_bad_credit = float('nan')  # F1 score can't be computed meaningfully\n",
    "        elif len(unique_class_labels) == 2:\n",
    "            f1_bad_credit = f1_score(true_labels_for_group, predicted_labels_for_group, pos_label=2, average='binary')  # F1 for binary classification (bad credit = 2)\n",
    "        else:\n",
    "            f1_bad_credit = f1_score(true_labels_for_group, predicted_labels_for_group, pos_label=2, average='macro')  # F1 for multiclass (fallback)\n",
    "\n",
    "        accuracy_for_group = accuracy_score(true_labels_for_group, predicted_labels_for_group)  # Accuracy for the group\n",
    "\n",
    "        confusion_matrix_for_group = confusion_matrix(true_labels_for_group, predicted_labels_for_group, labels=[1, 2])  # Confusion matrix for good/bad credit\n",
    "\n",
    "        # Pad confusion matrix if it's not 2x2 (e.g., missing one class)\n",
    "        if confusion_matrix_for_group.shape != (2, 2):\n",
    "            padded_confusion_matrix = np.zeros((2, 2), dtype=int)  # Create empty 2x2 matrix\n",
    "            for i, row_label in enumerate(np.unique(true_labels_for_group)):\n",
    "                for j, col_label in enumerate(np.unique(predicted_labels_for_group)):\n",
    "                    padded_confusion_matrix[row_label - 1, col_label - 1] = confusion_matrix_for_group[i, j]  # Fill in values\n",
    "            confusion_matrix_for_group = padded_confusion_matrix  # Replace with padded version\n",
    "\n",
    "        true_negatives, false_positives, false_negatives, true_positives = confusion_matrix_for_group.ravel()  # Unpack confusion matrix values\n",
    "\n",
    "        # Retrieve original label from encoded mapping\n",
    "        original_label_lookup = encoded_to_original_mapping.get(protected_attribute, {}).get(group_value)\n",
    "        if original_label_lookup is None:\n",
    "            readable_group_label = str(group_value)  # Fallback to raw value\n",
    "        else:\n",
    "            if isinstance(original_label_lookup, set):\n",
    "                readable_group_label = ', '.join(sorted(str(val) for val in original_label_lookup))  # Join multiple values\n",
    "            else:\n",
    "                readable_group_label = attribute_mapping_german_credit.get(\n",
    "                    protected_attribute.replace(\"_encoded\", \"\"), {}\n",
    "                ).get(original_label_lookup, str(original_label_lookup))  # Map to human-readable label\n",
    "\n",
    "        # Print performance metrics for the group\n",
    "        print(f\"\\nGroup {group_value} ({readable_group_label}):\")\n",
    "        print(f\"  Accuracy: {accuracy_for_group:.4f}\")\n",
    "        print(f\"  F1 Score (Bad Credit): {f1_bad_credit:.4f}\")\n",
    "        print(f\"  Confusion Matrix:\\n{confusion_matrix_for_group}\")\n",
    "        print(f\"    False Positives: {false_positives}\")\n",
    "        print(f\"    False Negatives: {false_negatives}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d14fb9-0b63-48a4-9d2c-b07d8dbd5ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store fairness metrics for each group\n",
    "fairness_metrics_by_group = []\n",
    "\n",
    "# Loop through each protected attribute (e.g., gender, age)\n",
    "for protected_attribute in protected_attributes:\n",
    "    \n",
    "    # Loop through each unique group within the protected attribute (e.g., male, female)\n",
    "    for group_value in X_test_with_protected[protected_attribute].unique():\n",
    "        \n",
    "        # Get indices of rows in the test set that belong to the current group\n",
    "        group_sample_indices = X_test_with_protected[X_test_with_protected[protected_attribute] == group_value].index\n",
    "        \n",
    "        # Extract true labels for the current group\n",
    "        true_labels_for_group = y_test.loc[group_sample_indices]\n",
    "        \n",
    "        # Extract predicted labels for the current group\n",
    "        predicted_labels_for_group = y_pred_series.loc[group_sample_indices]\n",
    "        \n",
    "        # Compute accuracy for the current group\n",
    "        group_accuracy = accuracy_score(true_labels_for_group, predicted_labels_for_group)\n",
    "        \n",
    "        # Compute F1 score for bad credit (label = 2) for the current group\n",
    "        group_f1_bad_credit = f1_score(true_labels_for_group, predicted_labels_for_group, pos_label=2)\n",
    "        \n",
    "        # Compute confusion matrix and unpack values\n",
    "        true_negatives, false_positives, false_negatives, true_positives = confusion_matrix(\n",
    "            true_labels_for_group, predicted_labels_for_group, labels=[1, 2]\n",
    "        ).ravel()\n",
    "\n",
    "        # Retrieve original label from encoded mapping, if available\n",
    "        original_group_label = encoded_to_original_mapping.get(protected_attribute, {}).get(group_value)\n",
    "        \n",
    "        # Determine readable label for the group\n",
    "        if original_group_label is None:\n",
    "            readable_group_label = str(group_value)  # Fallback to encoded value\n",
    "        else:\n",
    "            if isinstance(original_group_label, set):\n",
    "                readable_group_label = ', '.join(sorted(str(val) for val in original_group_label))  # Join multiple values\n",
    "            else:\n",
    "                readable_group_label = attribute_mapping_german_credit.get(\n",
    "                    protected_attribute.replace(\"_encoded\", \"\"), {}\n",
    "                ).get(original_group_label, str(original_group_label))  # Map to human-readable label\n",
    "\n",
    "        # Append computed metrics and metadata to the results list\n",
    "        fairness_metrics_by_group.append({\n",
    "            'protected_attribute': protected_attribute,\n",
    "            'group_value': group_value,\n",
    "            'group_label': readable_group_label,\n",
    "            'num_samples': len(group_sample_indices),\n",
    "            'accuracy': group_accuracy,\n",
    "            'f1_score': group_f1_bad_credit,\n",
    "            'false_positives': false_positives,\n",
    "            'false_negatives': false_negatives,\n",
    "            'true_positives': true_positives,\n",
    "            'true_negatives': true_negatives\n",
    "        })\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame for inspection and analysis\n",
    "fairness_metrics_df = pd.DataFrame(fairness_metrics_by_group)\n",
    "print(fairness_metrics_df)  # Display the fairness metrics for each group\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9331b228-9233-431d-8a2c-0bd168e6b0ed",
   "metadata": {},
   "source": [
    "# ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c26fa5-4a5a-4442-8221-2b7467f87c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual results of y_train data T's vs F's\n",
    "print(y_train.value_counts(normalize=True))\n",
    "\n",
    "# predicted results of y_pred data T's vs F's\n",
    "print(np.bincount(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63dce77-8fa7-4ec5-9d10-a1a2ad17e8d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98119af1-322c-4779-839b-eb4ed3c947af",
   "metadata": {},
   "source": [
    "# Profit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18afb30-c3ac-458b-afef-0a1b15f1d590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_profit(y_true, predicted_probs, approval_threshold):\n",
    "    profit = 0\n",
    "    for actual, prob in zip(y_true, predicted_probs):\n",
    "        predicted_approval = prob >= approval_threshold  # threshold on probability\n",
    "        if actual == 1 and predicted_approval:\n",
    "            profit += 10   # Approved, should approve\n",
    "        elif actual == 1 and not predicted_approval:\n",
    "            profit -= 5    # Approved, but denied\n",
    "        elif actual == 2 and predicted_approval:\n",
    "            profit -= 3    # Denied, but approved\n",
    "        # actual == 2 and denied -> 0 profit (no change)\n",
    "    return profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35f8674-0b88-46c3-9a04-186682eceec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_and_score(\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    base_credit_score=0,\n",
    "    high_risk_credit_score=100,\n",
    "    **logistic_kwargs\n",
    "):\n",
    "    # Train logistic regression\n",
    "    clf = LogisticRegression(**logistic_kwargs, random_state=42, max_iter=1000)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict classes and probabilities on test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_prob = clf.predict_proba(X_test)[:, 1]  # prob of class \"2\" (bad credit)\n",
    "\n",
    "    # Evaluate metrics if needed (accuracy, f1, confusion matrix) here or outside\n",
    "\n",
    "    # Instead of manual scorecard rules, you might map probabilities directly to scores:\n",
    "    # For example, scale prob between base_credit_score and high_risk_credit_score:\n",
    "    test_scores = base_credit_score + (high_risk_credit_score - base_credit_score) * y_prob\n",
    "\n",
    "    return clf, y_pred, y_prob, test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4579089d-ca68-416b-aff1-6a366ba69204",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_profit = float('-inf')\n",
    "best_threshold = None\n",
    "for threshold in range(0, 101):\n",
    "    profit = calculate_profit(y_test, y_prob, threshold)\n",
    "    if profit > best_profit:\n",
    "        best_profit = profit\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Best Profit: {best_profit} at Threshold: {best_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63160892-e1dc-4959-989c-bd41c0fae45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sweep over possible thresholds between 0 and 1 (or score range) to find best profit:\n",
    "best_profit = float('-inf')\n",
    "best_threshold = None\n",
    "\n",
    "for threshold in np.linspace(0, 1, 101):\n",
    "    profit = calculate_profit(y_test, y_prob, threshold)\n",
    "    if profit > best_profit:\n",
    "        best_profit = profit\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Best Profit: {best_profit} at Threshold: {best_threshold:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470e5be8-43ca-4f53-9c00-082f153e5b3a",
   "metadata": {},
   "source": [
    "#Debug Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae90c3b-5d47-4c30-afb6-7cf486ebc69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_profit_v2(y_true, predicted_probs, approval_threshold):\n",
    "    profit = 0\n",
    "    for actual, prob in zip(y_true, predicted_probs):\n",
    "        predicted_approval = prob >= approval_threshold  # threshold on probability\n",
    "        if actual == 1 and predicted_approval:\n",
    "            profit += 10   # Approved, should approve\n",
    "        elif actual == 1 and not predicted_approval:\n",
    "            profit -= 5    # Approved, but denied\n",
    "        elif actual == 2 and predicted_approval:\n",
    "            profit -= 10   # Denied, but approved\n",
    "        elif actual == 2 and not predicted_approval:\n",
    "            profit += 5    # Denied, and correctly denied\n",
    "    return profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fec9205-f93c-445b-9e29-25e8a8fc402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_profit = float('-inf')\n",
    "best_threshold = None\n",
    "\n",
    "# Finer granularity sweep\n",
    "for threshold in [x/100 for x in range(5, 21)]:\n",
    "    profit = calculate_profit_v2(y_test, y_prob, threshold)\n",
    "    if profit > best_profit:\n",
    "        best_profit = profit\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Best Profit: {best_profit} at Threshold: {best_threshold:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29583cc-dd21-4715-b77d-666f3af6a0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_prob, bins=20)\n",
    "plt.xlabel(\"Predicted Probability (Good Credit)\")\n",
    "plt.ylabel(\"Number of Samples\")\n",
    "plt.title(\"Distribution of Predicted Probabilities\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7c64b0-e4c7-455c-abe7-3f0bba536a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(classification_report(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3ab1ea-502b-44aa-b5e9-f971d1be5c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "for threshold in range(0, 101, 5):\n",
    "    profit = calculate_profit_v2(y_test, test_scores, threshold)\n",
    "    print(f\"Threshold: {threshold}, Profit: {profit}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60410583-de92-43d3-bd01-2de9d8c2e079",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [x/100 for x in range(5, 21)]\n",
    "profits = [calculate_profit_v2(y_test, y_prob, t) for t in thresholds]\n",
    "\n",
    "plt.plot(thresholds, profits, marker='o')\n",
    "plt.title(\"Profit vs. Threshold (0.05–0.20 Range)\")\n",
    "plt.xlabel(\"Approval Threshold\")\n",
    "plt.ylabel(\"Profit\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13233127-3cf2-440a-992e-e3cb9f4f3a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores = []\n",
    "for t in thresholds:\n",
    "    y_pred = (y_prob >= t).astype(int)\n",
    "    f1_scores.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "plt.plot(thresholds, f1_scores, marker='x', color='orange')\n",
    "plt.title(\"F1 Score vs. Threshold\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3032da9a-3d49-4279-82da-c2dcb8d37bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get precision, recall, and thresholds\n",
    "# Note: You must binarize the y_test for this to work correctly\n",
    "y_test_bin = (y_test == 2).astype(int)  # class \"2\" = bad credit\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test_bin, y_prob)\n",
    "\n",
    "# Calculate F1 scores for each threshold\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, precision[:-1], label=\"Precision\", color=\"green\", marker='o')\n",
    "plt.plot(thresholds, recall[:-1], label=\"Recall\", color=\"blue\", marker='s')\n",
    "plt.plot(thresholds, f1_scores[:-1], label=\"F1 Score\", color=\"orange\", marker='x')\n",
    "\n",
    "plt.title(\"Precision, Recall, and F1 Score vs. Threshold\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b4ca73-000e-4ec4-9e2f-b3dec900b62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.hist(test_scores, bins=50)\n",
    "plt.title(\"Histogram of Predicted Creditworthiness Scores\")\n",
    "plt.xlabel(\"Score (0-100)\")\n",
    "plt.ylabel(\"Number of test samples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81689c3-ca1d-4892-9401-94bd0328aae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([y_prob[y_test == 1], y_prob[y_test == 2]], bins=20, label=[\"Good Credit\", \"Bad Credit\"], stacked=True)\n",
    "plt.legend()\n",
    "plt.title(\"Score Distributions by Class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aacd57-f22d-4c9e-a4c7-c3a70d1d42df",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = clf.named_steps['logreg'].coef_[0] \n",
    "for feature, weight in sorted(zip(X_train.columns, coefs), key=lambda x: abs(x[1]), reverse=True):\n",
    "    print(f\"{feature}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff3396c-4245-44ed-ae94-3bdcee2cf49b",
   "metadata": {},
   "source": [
    "# Bias Mitigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c245a66-b853-4ee4-adf7-01eb7384dcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_merged)\n",
    "\n",
    "results_df = df_merged.loc[X_test_copy.index].copy()\n",
    "results_df['y_true'] = y_test\n",
    "results_df['y_pred'] = y_pred\n",
    "results_df['prob'] = y_prob\n",
    "\n",
    "print(results_df)\n",
    "\n",
    "# Define the correct mapping from numeric labels to binary sex\n",
    "label_sex_dict_cleaned = {\n",
    "    1: 1,  # male\n",
    "    2: 0,  # female\n",
    "    3: 1,  # male\n",
    "    4: 1,  # male\n",
    "    5: 0   # female\n",
    "}\n",
    "\n",
    "# Map the personal_status_sex_label to 0 or 1\n",
    "results_df['sex_binary'] = results_df['personal_status_sex_label'].map(label_sex_dict_cleaned)\n",
    "\n",
    "# Start with the numerical feature columns\n",
    "numerical_cols = X_test.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Add required columns\n",
    "numerical_cols += ['y_true', 'sex_binary']\n",
    "\n",
    "# Subset to only numerical data\n",
    "aif_df = results_df[numerical_cols].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b27742-9509-49d9-b43d-bf8bb7c8f8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric columns from X_test for fairness analysis\n",
    "numeric_feature_columns = X_test.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Add true labels and binary sex attribute to the list of columns\n",
    "numeric_feature_columns += ['y_true', 'sex_binary']\n",
    "\n",
    "# Create a subset DataFrame with only numeric columns and fairness-relevant attributes\n",
    "aif360_input_df = results_df[numeric_feature_columns].copy()\n",
    "\n",
    "# Create BinaryLabelDataset for AIF360 using true labels and protected attribute\n",
    "aif360_true_label_dataset = BinaryLabelDataset(\n",
    "    df=aif360_input_df,  # DataFrame with features and protected attribute\n",
    "    label_names=['y_true'],  # Column containing true labels\n",
    "    protected_attribute_names=['sex_binary'],  # Column containing protected attribute\n",
    "    favorable_label=1,  # Label for favorable outcome (e.g., good credit)\n",
    "    unfavorable_label=2  # Label for unfavorable outcome (e.g., bad credit)\n",
    ")\n",
    "\n",
    "# Copy the dataset to assign predicted labels and scores\n",
    "aif360_predicted_dataset = aif360_true_label_dataset.copy()\n",
    "\n",
    "# Assign predicted labels to the copied dataset\n",
    "aif360_predicted_dataset.labels = results_df['y_pred'].values.reshape(-1, 1)\n",
    "\n",
    "# Assign predicted probabilities to the copied dataset\n",
    "aif360_predicted_dataset.scores = results_df['prob'].values.reshape(-1, 1)\n",
    "\n",
    "# Define privileged group (male) for fairness evaluation\n",
    "privileged_group_definition = [{'sex_binary': 1}]\n",
    "\n",
    "# Define unprivileged group (female) for fairness evaluation\n",
    "unprivileged_group_definition = [{'sex_binary': 0}]\n",
    "\n",
    "# Instantiate ClassificationMetric to evaluate fairness between groups\n",
    "fairness_metric_evaluator = ClassificationMetric(\n",
    "    aif360_true_label_dataset,  # Dataset with true labels\n",
    "    aif360_predicted_dataset,   # Dataset with predicted labels and scores\n",
    "    privileged_groups=privileged_group_definition,  # Privileged group definition\n",
    "    unprivileged_groups=unprivileged_group_definition  # Unprivileged group definition\n",
    ")\n",
    "\n",
    "# Print key fairness metrics computed by AIF360\n",
    "print(\"Disparate Impact:\", fairness_metric_evaluator.disparate_impact())  # Ratio of favorable outcomes\n",
    "print(\"Statistical Parity Difference:\", fairness_metric_evaluator.statistical_parity_difference())  # Difference in approval rates\n",
    "print(\"Equal Opportunity Difference:\", fairness_metric_evaluator.equal_opportunity_difference())  # Difference in true positive rates\n",
    "print(\"Average Odds Difference:\", fairness_metric_evaluator.average_odds_difference())  # Difference in both TPR and FPR\n",
    "print(\"Equalized Odds Difference (TPR - FPR):\", \n",
    "      fairness_metric_evaluator.equal_opportunity_difference() - fairness_metric_evaluator.false_positive_rate_difference())  # Net difference in TPR and FPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c28926-5ae5-4f1d-9b8d-be4ec5fb410a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the two fairness metrics\n",
    "disparate_impact = metric.disparate_impact()\n",
    "stat_parity_diff = metric.statistical_parity_difference()\n",
    "\n",
    "# Put metrics into a DataFrame for plotting\n",
    "fairness_df = pd.DataFrame({\n",
    "    'Metric': ['Disparate Impact', 'Statistical Parity Difference'],\n",
    "    'Value': [disparate_impact, stat_parity_diff]\n",
    "})\n",
    "\n",
    "# Define fairness thresholds for reference\n",
    "thresholds = {\n",
    "    'Disparate Impact': (0.8, 1.25),\n",
    "    'Statistical Parity Difference': (-0.1, 0.1)\n",
    "}\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(data=fairness_df, x='Metric', y='Value', palette='coolwarm')\n",
    "plt.axhline(0, color='gray', linewidth=1)\n",
    "\n",
    "# Add threshold lines\n",
    "for metric_name, (low, high) in thresholds.items():\n",
    "    if metric_name == 'Disparate Impact':\n",
    "        plt.axhline(low, color='green', linestyle='--', label='Acceptable Range' if metric_name == 'Disparate Impact' else \"\")\n",
    "        plt.axhline(high, color='green', linestyle='--')\n",
    "    else:\n",
    "        plt.axhline(low, color='purple', linestyle='--', label='Acceptable Range' if metric_name == 'Statistical Parity Difference' else \"\")\n",
    "        plt.axhline(high, color='purple', linestyle='--')\n",
    "\n",
    "plt.title('Fairness Metrics: Disparate Impact & Statistical Parity Difference')\n",
    "plt.ylim(-1, 2)  # Adjust range as needed\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25d79b2-9193-4ecb-a47f-2469c45b9765",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
